"""Run logging and report generation utilities."""
from __future__ import annotations

import json
import secrets
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Mapping, Sequence

from pentesting_rl.metrics import compute_metrics
from pentesting_rl.novelty_tokens import summarize_paths_top
from pentesting_rl.state_model import estimate_info_gain, state_signature


_SNIPPET_LIMIT = 500
_BLOB_CHAR_LIMIT = 4096
_BLOB_LINE_LIMIT = 200


class RunIdGenerator:
    @staticmethod
    def generate() -> str:
        timestamp = time.strftime("%Y%m%dT%H%M%SZ", time.gmtime())
        rand = secrets.token_hex(3)
        return f"{timestamp}_{rand}"


@dataclass
class RunMetadata:
    run_id: str
    target: dict[str, object]
    environment: dict[str, object]
    run_dir: Path
    started_at: float
    errors: list[dict[str, object]] = field(default_factory=list)
    findings: list[dict[str, object]] = field(default_factory=list)


class RunWriter:
    def __init__(self, run_root: str | Path, run_id: str | None = None) -> None:
        self.run_id = run_id or RunIdGenerator.generate()
        self.run_dir = Path(run_root) / self.run_id
        self.run_dir.mkdir(parents=True, exist_ok=True)
        self.runlog_path = self.run_dir / "runlog.jsonl"
        self._line_index = 0
        self._prepare_artifacts()

    def _prepare_artifacts(self) -> None:
        for subdir in ("stdout", "stderr", "http", "files"):
            (self.run_dir / "artifacts" / subdir).mkdir(parents=True, exist_ok=True)

    def append_step(self, record: dict[str, object]) -> int:
        runlog_line = self._line_index
        record.setdefault("runlog_line", runlog_line)
        record.setdefault("step_index", runlog_line)
        with self.runlog_path.open("a", encoding="utf-8") as handle:
            handle.write(json.dumps(record, ensure_ascii=True) + "\n")
        self._line_index += 1
        return runlog_line

    def write_text_artifact(
        self, kind: str, step_index: int, content: str
    ) -> tuple[str | None, list[dict[str, object]]]:
        errors: list[dict[str, object]] = []
        if kind not in {"stdout", "stderr", "http"}:
            kind = "files"
        filename = f"step_{step_index:04d}.txt"
        if kind == "http":
            filename = f"step_{step_index:04d}_resp.txt"
        path = self.run_dir / "artifacts" / kind / filename
        try:
            path.write_text(content, encoding="utf-8")
        except OSError as exc:
            errors.append(
                {
                    "type": "artifact_write_failed",
                    "artifact_kind": kind,
                    "path": str(path.relative_to(self.run_dir)),
                    "message": str(exc),
                    "recover": "stored_snippet_only",
                }
            )
            return None, errors
        return str(path.relative_to(self.run_dir)), errors


def should_store_blob(text: str) -> bool:
    if len(text) > _BLOB_CHAR_LIMIT:
        return True
    line_count = text.count("\n") + 1 if text else 0
    return line_count > _BLOB_LINE_LIMIT


def snippet(text: str) -> str:
    if not text:
        return ""
    return text[:_SNIPPET_LIMIT]


def build_summary(steps_count: int, candidate_count: int, duration: float, paths_top: list[str]) -> dict[str, object]:
    return {
        "template": "자동화 에이전트가 대상에 대해 정찰 및 취약점 후보를 수집하고 보고서를 생성함.",
        "facts": {
            "steps": steps_count,
            "candidate_findings": candidate_count,
            "duration_sec": round(duration, 3),
            "unique_paths_top": paths_top,
        },
    }


def collect_paths_top(knowledge: Mapping[str, Sequence[str]]) -> list[str]:
    paths: list[str] = []
    for key in (
        "PATH_HINT",
        "LINK_FOUND",
        "FORM_ACTION",
        "SCRIPT_SRC",
        "JS_ENDPOINT",
        "ROBOTS_DISALLOW",
        "ROBOTS_ALLOW",
        "SITEMAP_URL",
    ):
        paths.extend([str(v) for v in knowledge.get(key, [])])
    return summarize_paths_top(paths)


def build_report(
    metadata: RunMetadata,
    knowledge_final: Mapping[str, Sequence[str]],
    steps_count: int,
    finished_at: float,
    schema_version: str = "0.1.0",
    generator_version: str = "mvp",
) -> dict[str, object]:
    duration = max(0.0, finished_at - metadata.started_at)
    summary = build_summary(
        steps_count,
        len(metadata.findings),
        duration,
        collect_paths_top(knowledge_final),
    )
    return {
        "schema_version": schema_version,
        "generator_version": generator_version,
        "run_id": metadata.run_id,
        "target": metadata.target,
        "environment": metadata.environment,
        "summary": summary,
        "steps_count": steps_count,
        "findings": metadata.findings,
        "knowledge_final": {key: list(values) for key, values in knowledge_final.items()},
        "errors": metadata.errors,
        "started_at": metadata.started_at,
        "finished_at": finished_at,
    }


def write_report(report_path: Path, payload: dict[str, object]) -> None:
    report_path.write_text(json.dumps(payload, indent=2, ensure_ascii=True), encoding="utf-8")


def generate_reports(
    report_dir: str | Path,
    history: list[dict[str, object]],
    knowledge_store: dict[str, list[str]],
    window: int = 5,
) -> dict[str, Path]:
    report_path = Path(report_dir)
    report_path.mkdir(parents=True, exist_ok=True)

    run_log = build_run_log(history)
    graph = build_transition_graph(history, window=window)
    knowledge_snapshot = build_knowledge_snapshot(knowledge_store, graph)
    metrics = compute_metrics(history, knowledge_store, window=window)
    report_text = build_markdown_report(metrics, knowledge_snapshot, graph)

    run_file = report_path / "run.json"
    knowledge_file = report_path / "knowledge.json"
    graph_file = report_path / "graph.json"
    report_file = report_path / "report.md"

    _write_json(run_file, run_log)
    _write_json(knowledge_file, knowledge_snapshot)
    _write_json(graph_file, graph)
    report_file.write_text(report_text, encoding="utf-8")

    return {
        "run": run_file,
        "knowledge": knowledge_file,
        "graph": graph_file,
        "report": report_file,
    }


def build_run_log(history: list[dict[str, object]]) -> dict[str, object]:
    entries: list[dict[str, object]] = []
    for idx, transition in enumerate(history):
        meta = transition.get("meta", {}) or {}
        action = transition.get("a_t", {}) or {}
        state = transition.get("s_t", {}) or {}
        entry = {
            "step": meta.get("step", idx),
            "episode": meta.get("episode"),
            "tool": action.get("tool"),
            "options": action.get("options"),
            "command": transition.get("command"),
            "reward": action.get("reward"),
            "external_reward": action.get("external_reward"),
            "intrinsic_reward": action.get("intrinsic_reward"),
            "cost_penalty": action.get("cost_penalty"),
            "cost": state.get("cost", {}),
            "safety": state.get("safety", {}),
            "new_kk": sorted(list(state.get("updated_kk", []) or [])),
            "updates_detail": transition.get("updates_detail", {}),
        }
        entries.append(entry)
    return {"steps": entries}


def build_knowledge_snapshot(
    knowledge_store: dict[str, list[str]],
    graph: dict[str, object],
) -> dict[str, object]:
    artifacts = {
        "endpoints": _collect_unique(
            knowledge_store,
            [
                "PATH_HINT",
                "LINK_FOUND",
                "FORM_ACTION",
                "SCRIPT_SRC",
                "JS_ENDPOINT",
                "ROBOTS_DISALLOW",
                "ROBOTS_ALLOW",
                "SITEMAP_URL",
            ],
        ),
        "params": _collect_unique(knowledge_store, ["PARAM_DETECTED"]),
        "cookies": _collect_unique(knowledge_store, ["COOKIE_SEEN", "HTTP_SET_COOKIE"]),
    }
    return {
        "kk_store": {key: list(values) for key, values in knowledge_store.items()},
        "artifacts": artifacts,
        "state_graph": {
            "node_count": len(graph.get("nodes", [])),
            "edge_count": len(graph.get("edges", [])),
        },
    }


def build_transition_graph(
    history: list[dict[str, object]],
    window: int = 5,
) -> dict[str, object]:
    nodes: dict[str, dict[str, object]] = {"START": {"summary": {"type": "start"}}}
    edges: list[dict[str, object]] = []

    seen_paths: set[str] = set()
    seen_params: set[str] = set()
    seen_cookies: set[str] = set()

    prev_id = "START"
    for idx, transition in enumerate(history):
        state_id, summary = state_signature(history, idx, window=window)
        if state_id not in nodes:
            nodes[state_id] = {"summary": summary}

        gain, new_paths, new_params, new_cookies = estimate_info_gain(
            transition, seen_paths, seen_params, seen_cookies
        )
        action = transition.get("a_t", {}) or {}
        state = transition.get("s_t", {}) or {}
        meta = transition.get("meta", {}) or {}
        edges.append(
            {
                "from": prev_id,
                "to": state_id,
                "step": meta.get("step", idx),
                "tool": action.get("tool"),
                "options": action.get("options"),
                "reward": action.get("reward"),
                "cost": state.get("cost", {}),
                "info_gain": gain,
                "new_paths": new_paths,
                "new_params": new_params,
                "new_cookies": new_cookies,
            }
        )
        prev_id = state_id

    return {
        "nodes": [{"id": node_id, **payload} for node_id, payload in nodes.items()],
        "edges": edges,
        "window": window,
    }


def build_markdown_report(
    metrics: dict[str, object],
    knowledge_snapshot: dict[str, object],
    graph: dict[str, object],
) -> str:
    artifacts = knowledge_snapshot.get("artifacts", {})
    endpoints = artifacts.get("endpoints", [])
    params = artifacts.get("params", [])
    cookies = artifacts.get("cookies", [])

    lines = [
        "# HTB Web RL Report",
        "",
        "## Overview",
        f"- Total endpoints: {len(endpoints)}",
        f"- Total params: {len(params)}",
        f"- Total cookies: {len(cookies)}",
        f"- State nodes: {len(graph.get('nodes', []))}",
        f"- State edges: {len(graph.get('edges', []))}",
        "",
        "## Metrics",
        f"- IG/request: {metrics.get('ig_per_request')}",
        f"- Steps to first transition: {metrics.get('steps_to_first_transition')}",
        f"- Steps to deep state: {metrics.get('steps_to_deep_state')}",
        "",
        "## Observed Surfaces (sample)",
        f"- Endpoints: {', '.join(endpoints[:10])}",
        f"- Params: {', '.join(params[:10])}",
        f"- Cookies: {', '.join(cookies[:10])}",
        "",
        "## State Transitions",
        "- This report focuses on observed transitions without vulnerability labels.",
        "",
        "## Cost Summary",
        f"- Total requests: {metrics.get('total_requests')}",
        "",
        "## Policy Trace (sample)",
        "- See run.json for full step-by-step actions.",
        "",
    ]
    return "\n".join(lines)


def _collect_unique(store: dict[str, list[str]], keys: list[str]) -> list[str]:
    values: set[str] = set()
    for key in keys:
        for value in store.get(key, []) or []:
            values.add(str(value))
    return sorted(values)


def _write_json(path: Path, payload: dict[str, object]) -> None:
    path.write_text(json.dumps(payload, indent=2, ensure_ascii=True), encoding="utf-8")
