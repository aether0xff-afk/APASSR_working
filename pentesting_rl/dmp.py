"""Decision Making Process orchestrating the RL loop."""
from __future__ import annotations

import pathlib
import random
import sys
import time
from dataclasses import dataclass, field
import math
import shlex
from typing import Callable, Dict, List, Optional, Tuple

if __package__ in {None, ""}:
    sys.path.append(str(pathlib.Path(__file__).resolve().parent.parent))

from pentesting_rl.knowledge import KnowledgeStorage, KKEncoder
from pentesting_rl.parser import NmapXMLParser
from pentesting_rl.policy import LearnablePolicy, PolicyOption
from pentesting_rl.prophecy import ProphecyModel
from pentesting_rl.reward import RewardCalculator


CommandExecutor = Callable[[str], Tuple[str, bool]]  # returns xml, error_flag

FAMOUS_20_PORTS = [21, 22, 23, 25, 53, 80, 110, 111, 135, 139, 143, 443, 445, 993, 995, 1433, 1521, 3306, 3389, 8080]
DEFAULT_TOP_PORTS_CHOICES = [20, 50, 100, 200, 500, 1000]
DEFAULT_SCRIPT_SET_CHOICES = [
    "default",
    "safe",
    "http-*",
    "banner",
    "version",
    "all",
    "auth",
    "brute",
    "discovery",
    "vuln",
    "exploit",
    "dos",
    "broadcast",
    "external",
    "fuzzer",
    "intrusive",
    "malware",
]


@dataclass
class DMPConfig:
    max_steps: int = 50
    beta: float = 0.3
    lambda_err: float = 0.5
    target_ip: str = "127.0.0.1"
    prophecy_enabled: bool = True
    imagination_enabled: bool = False
    imagination_rollouts: int = 3
    learning_enabled: bool = True


@dataclass
class DMPState:
    step: int = 0
    episode: int = 0
    history: List[Dict[str, object]] = field(default_factory=list)
    flag_log: List[Dict[str, object]] = field(default_factory=list)


class DecisionMakingProcess:
    """Control loop for selecting, executing, and learning from actions."""

    def __init__(
        self,
        executor: Optional[CommandExecutor] = None,
        config: DMPConfig | None = None,
    ) -> None:
        self.config = config or DMPConfig()
        self.knowledge = KnowledgeStorage()
        self.policy = LearnablePolicy()
        self.reward_calc = RewardCalculator(beta=self.config.beta)
        self.prophecy = ProphecyModel(lambda_err=self.config.lambda_err)
        self.encoder = KKEncoder()
        self.parser = NmapXMLParser(self.knowledge)
        self.state = DMPState()

        self._executor: CommandExecutor = executor or self._dummy_executor
        self._last_reset_time = time.time()

    def build_command(
        self, a: PolicyOption, b: PolicyOption, c: PolicyOption
    ) -> Tuple[str, Dict[str, str]]:
        parts: List[str] = ["nmap"]
        params: Dict[str, str] = {}
        for option in (a, b, c):
            if option.parameters:
                for kk, value in option.parameters.items():
                    resolved = self._resolve_param(kk, value)
                    if resolved:
                        params[kk] = resolved
                        parts.append(resolved)
        parts.append(self.config.target_ip)
        parts.extend(["-oX", "-"])
        command = " ".join(parts)
        return command, params

    def step(self, executor: Optional[CommandExecutor] = None) -> Dict[str, object]:
        if executor is None:
            executor = self.executor
        (a, b, c), imagination_info = self._select_action()
        command, params = self.build_command(a, b, c)
        xml_output, exec_error = executor(command)
        parse_result = self.parser.parse_and_update(xml_output)
        script_updates, script_new_values = self._update_script_knowledge(command)
        combined_updates = set(parse_result.updates) | script_updates
        combined_new_values = dict(parse_result.new_values)
        for key, values in script_new_values.items():
            combined_new_values.setdefault(key, []).extend(values)

        # External reward
        combo_key = "/".join((a.name, b.name, c.name))
        flag_found = "Flag" in combined_updates and bool(
            self.knowledge.store.get("Flag")
        )
        external = self.reward_calc.external_reward(
            combo_key,
            syntax_error=exec_error,
            crash_error=False,
            kk_updated=combined_updates,
            flag_found=flag_found,
        )

        # Intrinsic reward from prophecy model
        update_vec = self.encoder.encode(combined_updates)
        if self.config.prophecy_enabled:
            loss, _, _ = self.prophecy.update(update_vec, parse_result.error)
            intrinsic = self.reward_calc.intrinsic_reward(loss)
        else:
            loss = 0.0
            intrinsic = 0.0
        total_reward = self.reward_calc.total_reward(external, intrinsic)

        # Policy update
        if self.config.learning_enabled:
            self.policy.update(total_reward, (a, b, c))

        knowledge_snapshot = {
            key: list(values) for key, values in self.knowledge.store.items() if values
        }
        policy_snapshot = self.policy.describe()

        # Log transition
        transition = {
            "s_t": {
                "updated_kk": combined_updates,
                "error": parse_result.error,
            },
            "a_t": {
                "options": (a.name, b.name, c.name),
                "params": params,
                "reward": total_reward,
                "external_reward": external,
                "intrinsic_reward": intrinsic,
                "prophecy_loss": loss,
            },
            "s_t+1": {
                "updated_kk": combined_updates,
                "error": parse_result.error,
            },
            "flag_found": flag_found,
            "command": command,
            "updates_detail": combined_new_values,
            "knowledge_snapshot": knowledge_snapshot,
            "policy_snapshot": policy_snapshot,
            "imagination": imagination_info,
            "meta": {"episode": self.state.episode, "step": self.state.step},
        }
        self.state.history.append(transition)
        self.state.step += 1

        if flag_found:
            self._record_success()
            self._reset_episode()

        if self.state.step >= self.config.max_steps:
            self._reset_episode()

        return transition

    def _select_action(
        self,
    ) -> Tuple[Tuple[PolicyOption, PolicyOption, PolicyOption], Dict[str, object]]:
        """Select an action, optionally using a lightweight imagination rollout."""

        if not self.config.imagination_enabled or self.config.imagination_rollouts <= 1:
            return self.policy.sample(), {
                "mode": "policy",
                "rollouts": 0,
            }

        best_reward = -float("inf")
        best_choice: Tuple[PolicyOption, PolicyOption, PolicyOption] | None = None
        rollout_details: List[Dict[str, object]] = []
        for _ in range(self.config.imagination_rollouts):
            candidate = self.policy.sample()
            reward_estimate = self._estimate_expected_reward(candidate)
            rollout_details.append(
                {
                    "options": tuple(opt.name for opt in candidate),
                    "estimated_reward": reward_estimate,
                }
            )
            if reward_estimate > best_reward:
                best_reward = reward_estimate
                best_choice = candidate

        return best_choice or self.policy.sample(), {
            "mode": "imagination",
            "rollouts": self.config.imagination_rollouts,
            "candidates": rollout_details,
            "best_estimate": best_reward,
        }

    def _estimate_expected_reward(
        self, candidate: Tuple[PolicyOption, PolicyOption, PolicyOption]
    ) -> float:
        """Estimate reward for a candidate without executing it.

        This favors novel combos (via the curiosity term) and leans on prophecy
        probabilities to avoid combinations likely to be erroneous.
        """

        combo_key = "/".join(opt.name for opt in candidate)
        count = self.reward_calc.combo_counts.get(combo_key, 0) + 1
        curiosity = 1 / math.sqrt(count)

        if self.config.prophecy_enabled:
            kk_probs, err_prob = self.prophecy.predict([])
            avg_update_prob = sum(kk_probs) / len(kk_probs) if kk_probs else 0.0
            expected_reward = curiosity + 2.0 * avg_update_prob
            expected_reward -= 5.0 * err_prob
        else:
            expected_reward = curiosity
        return expected_reward

    def run_episode(
        self, steps: Optional[int] = None, executor: Optional[CommandExecutor] = None
    ) -> None:
        target_steps = steps or self.config.max_steps
        for _ in range(target_steps):
            self.step(executor=executor)

    def _record_success(self) -> None:
        elapsed = time.time() - self._last_reset_time
        self.state.flag_log.append(
            {
                "episode": self.state.episode,
                "step": self.state.step,
                "elapsed_sec": elapsed,
                "history_len": len(self.state.history),
            }
        )

    def _reset_episode(self) -> None:
        self.knowledge.reset()
        self.reward_calc.reset_episode()
        self.state.step = 0
        self.state.episode += 1
        self.state.history.clear()
        self._last_reset_time = time.time()

    @property
    def executor(self) -> CommandExecutor:
        return self._executor

    @executor.setter
    def executor(self, func: CommandExecutor) -> None:
        self._executor = func

    def _resolve_param(self, kk: str, value: str) -> str:
        """Resolve a policy parameter using knowledge and fallbacks for placeholders."""

        def _fetch(key: str) -> str:
            stored = self.knowledge.store.get(key, [])
            if stored:
                return stored[-1]
            return self._fallback_for(key)

        resolved = value
        placeholders = [p for p in resolved.split() if p.startswith("{") and p.endswith("}")]
        # Replace each placeholder token individually to keep mixed values intact
        for token in placeholders:
            key = token.strip("{}")
            replacement = _fetch(key)
            resolved = resolved.replace(token, replacement)

        if resolved.startswith("{") and resolved.endswith("}"):
            # Entire token is a placeholder but was not replaced (no data)
            key = resolved.strip("{}")
            return _fetch(key)
        return resolved

    def _fallback_for(self, key: str) -> str:
        key_upper = key.upper()
        if key_upper in {"TARGET_IP", "TARGET"}:
            return self.config.target_ip
        if key_upper in {"PORT_LIST", "PORT_SPEC"}:
            return ",".join(str(p) for p in FAMOUS_20_PORTS)
        if key_upper == "TOP_PORTS":
            return str(random.choice(DEFAULT_TOP_PORTS_CHOICES))
        if key_upper == "SCRIPT_SET":
            return random.choice(DEFAULT_SCRIPT_SET_CHOICES)
        if key_upper == "PATH_HINT":
            return "/robots.txt"
        if key_upper == "MAX_RETRIES":
            return "2"
        if key_upper == "HOST_TIMEOUT":
            return "30s"
        if key_upper == "SCAN_DELAY":
            return "0ms"
        if key_upper == "MIN_RATE":
            return "0"
        if key_upper == "MAX_RATE":
            return "0"
        if key_upper == "VER_INTENSITY":
            return "7"
        return ""

    def _update_script_knowledge(
        self, command: str
    ) -> Tuple[set[str], Dict[str, List[str]]]:
        script_sets = self._extract_script_sets(command)
        if not script_sets:
            return set(), {}
        mapping = {"Script_Set": script_sets, "SCRIPT_SET": script_sets}
        return self.knowledge.update_many_with_values(mapping)

    @staticmethod
    def _extract_script_sets(command: str) -> List[str]:
        script_sets: List[str] = []
        tokens = shlex.split(command)
        idx = 0
        while idx < len(tokens):
            token = tokens[idx]
            if token == "--script" and idx + 1 < len(tokens):
                script_sets.append(tokens[idx + 1])
                idx += 2
                continue
            if token.startswith("--script="):
                script_sets.append(token.split("=", 1)[1])
            idx += 1
        return script_sets

    @staticmethod
    def _dummy_executor(command: str) -> Tuple[str, bool]:
        """Default executor that returns an empty XML envelope."""
        xml = """<nmaprun><host><status state='up'/><address addr='127.0.0.1'/></host></nmaprun>"""
        return xml, False
