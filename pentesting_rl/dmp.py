"""Decision Making Process orchestrating the RL loop."""
from __future__ import annotations

import pathlib
import random
import sys
import time
from dataclasses import dataclass, field
import math
from typing import Callable, Dict, List, Optional, Tuple

if __package__ in {None, ""}:
    sys.path.append(str(pathlib.Path(__file__).resolve().parent.parent))

from pentesting_rl.knowledge import KnowledgeStorage, KKEncoder
from pentesting_rl.policy import PolicyOption, ToolPolicyManager
from pentesting_rl.prophecy import ProphecyModel
from pentesting_rl.reward import RewardCalculator, RewardConfig
from pentesting_rl.tools import ToolAdapter, ToolRegistry


CommandExecutor = Callable[[str], Tuple[str, bool]]  # returns output, error_flag

FAMOUS_20_PORTS = [21, 22, 23, 25, 53, 80, 110, 111, 135, 139, 143, 443, 445, 993, 995, 1433, 1521, 3306, 3389, 8080]
DEFAULT_TOP_PORTS_CHOICES = [20, 50, 100, 200, 500, 1000]
DEFAULT_SCRIPT_SET_CHOICES = [
    "default",
    "safe",
    "http-*",
    "banner",
    "version",
    "all",
    "auth",
    "brute",
    "discovery",
    "vuln",
    "exploit",
    "dos",
    "broadcast",
    "external",
    "fuzzer",
    "intrusive",
    "malware",
]


@dataclass
class DMPConfig:
    max_steps: int = 50
    beta: float = 0.3
    lambda_err: float = 0.5
    lambda_cost: float = 0.05
    target_ip: str = "127.0.0.1"
    tool_name: str = "nmap"
    prophecy_enabled: bool = True
    imagination_enabled: bool = False
    imagination_rollouts: int = 3
    learning_enabled: bool = True
    reward_flags: RewardConfig = field(default_factory=RewardConfig)


@dataclass
class DMPState:
    step: int = 0
    episode: int = 0
    history: List[Dict[str, object]] = field(default_factory=list)
    flag_log: List[Dict[str, object]] = field(default_factory=list)


class DecisionMakingProcess:
    """Control loop for selecting, executing, and learning from actions."""

    def __init__(
        self,
        executor: Optional[CommandExecutor] = None,
        config: DMPConfig | None = None,
        tool: ToolAdapter | None = None,
    ) -> None:
        self.config = config or DMPConfig()
        self.knowledge = KnowledgeStorage()
        self.reward_calc = RewardCalculator(
            beta=self.config.beta, lambda_cost=self.config.lambda_cost
        )
        self.prophecy = ProphecyModel(lambda_err=self.config.lambda_err)
        self.encoder = KKEncoder()
        self.state = DMPState()
        self.tool_registry = ToolRegistry(self.knowledge)
        self.policy = ToolPolicyManager(
            policies={
                name: ToolPolicyManager._default_policy_for(name)
                for name in self.tool_registry.available()
            }
        )
        if tool is not None:
            self.tool = tool
        elif self.config.tool_name != "auto":
            self.tool = self.tool_registry.get(self.config.tool_name)
        else:
            self.tool = self.tool_registry.get("nmap")

        self._executor: CommandExecutor = executor or self._dummy_executor
        self._last_reset_time = time.time()
        self._step_hook: Optional[Callable[[Dict[str, object]], None]] = None

    def build_command(
        self, a: PolicyOption, b: PolicyOption, c: PolicyOption
    ) -> Tuple[str, Dict[str, str]]:
        command = self.tool.build_command((a, b, c), self.config.target_ip, self._resolve_param)
        return command.command, command.params

    def step(self, executor: Optional[CommandExecutor] = None) -> Dict[str, object]:
        if executor is None:
            executor = self.executor
        tool_name, (a, b, c), imagination_info = self._select_action()
        self.tool = self.tool_registry.get(tool_name)
        command, params = self.build_command(a, b, c)
        tool_output, exec_error = executor(command)
        parse_result = self.tool.parse_output(tool_output)
        tool_updates, tool_new_values = self.tool.update_knowledge_from_command(command, params)
        combined_updates = set(parse_result.updates) | tool_updates
        combined_new_values = dict(parse_result.new_values)
        for key, values in tool_new_values.items():
            combined_new_values.setdefault(key, []).extend(values)

        # External reward
        combo_key = "/".join((a.name, b.name, c.name))
        flag_found = "Flag" in combined_updates and bool(
            self.knowledge.store.get("Flag")
        )

        # Intrinsic reward from prophecy model
        update_vec = self.encoder.encode(combined_updates)
        if self.config.prophecy_enabled:
            loss, _, _ = self.prophecy.update(update_vec, parse_result.error)
        else:
            loss = 0.0
        reward_breakdown = self.reward_calc.compute_breakdown(
            combo_key,
            syntax_error=exec_error,
            crash_error=False,
            kk_updated=combined_updates,
            flag_found=flag_found,
            cost=parse_result.cost,
            prophecy_loss=loss,
            config=self.config.reward_flags,
        )
        total_reward = reward_breakdown["total"]

        # Policy update
        if self.config.learning_enabled:
            self.policy.update(total_reward, tool_name, (a, b, c))

        knowledge_snapshot = {
            key: list(values) for key, values in self.knowledge.store.items() if values
        }
        policy_snapshot = self.policy.describe(tool_name)

        # Log transition
        transition = {
            "s_t": {
                "updated_kk": combined_updates,
                "error": parse_result.error,
                "cost": parse_result.cost,
                "safety": parse_result.safety,
            },
            "a_t": {
                "options": (a.name, b.name, c.name),
                "params": params,
                "tool": tool_name,
                "reward": total_reward,
                "reward_breakdown": reward_breakdown,
                "prophecy_loss": loss,
            },
            "s_t+1": {
                "updated_kk": combined_updates,
                "error": parse_result.error,
            },
            "flag_found": flag_found,
            "command": command,
            "output": tool_output,
            "updates_detail": combined_new_values,
            "knowledge_snapshot": knowledge_snapshot,
            "policy_snapshot": policy_snapshot,
            "imagination": imagination_info,
            "meta": {"episode": self.state.episode, "step": self.state.step},
        }
        self.state.history.append(transition)
        if self._step_hook:
            self._step_hook(
                {
                    "step_index": self.state.step,
                    "tool": tool_name,
                    "options": (a.name, b.name, c.name),
                    "params": params,
                    "command": command,
                    "output": tool_output,
                    "exec_error": exec_error,
                    "parse_error": parse_result.error,
                    "updates": combined_updates,
                    "new_values": combined_new_values,
                    "reward_breakdown": reward_breakdown,
                    "knowledge": knowledge_snapshot,
                }
            )
        self.state.step += 1

        if flag_found:
            self._record_success()
            self._reset_episode()

        if self.state.step >= self.config.max_steps:
            self._reset_episode()

        return transition

    def _select_action(
        self,
    ) -> Tuple[str, Tuple[PolicyOption, PolicyOption, PolicyOption], Dict[str, object]]:
        """Select an action, optionally using a lightweight imagination rollout."""

        if not self.config.imagination_enabled or self.config.imagination_rollouts <= 1:
            tool_name, choice = self._sample_tool_action()
            return tool_name, choice, {
                "mode": "policy",
                "rollouts": 0,
            }

        best_reward = -float("inf")
        best_choice: Tuple[PolicyOption, PolicyOption, PolicyOption] | None = None
        best_tool: str | None = None
        rollout_details: List[Dict[str, object]] = []
        for _ in range(self.config.imagination_rollouts):
            tool_name, candidate = self._sample_tool_action()
            reward_estimate = self._estimate_expected_reward(tool_name, candidate)
            rollout_details.append(
                {
                    "tool": tool_name,
                    "options": tuple(opt.name for opt in candidate),
                    "estimated_reward": reward_estimate,
                }
            )
            if reward_estimate > best_reward:
                best_reward = reward_estimate
                best_tool = tool_name
                best_choice = candidate

        fallback_tool, fallback_choice = self._sample_tool_action()
        return best_tool or fallback_tool, best_choice or fallback_choice, {
            "mode": "imagination",
            "rollouts": self.config.imagination_rollouts,
            "candidates": rollout_details,
            "best_estimate": best_reward,
        }

    def _estimate_expected_reward(
        self, tool_name: str, candidate: Tuple[PolicyOption, PolicyOption, PolicyOption]
    ) -> float:
        """Estimate reward for a candidate without executing it.

        This favors novel combos (via the curiosity term) and leans on prophecy
        probabilities to avoid combinations likely to be erroneous.
        """

        combo_key = "/".join(opt.name for opt in candidate)
        count = self.reward_calc.combo_counts.get(combo_key, 0) + 1
        curiosity = 1 / math.sqrt(count)

        if self.config.prophecy_enabled:
            kk_probs, err_prob = self.prophecy.predict([])
            avg_update_prob = sum(kk_probs) / len(kk_probs) if kk_probs else 0.0
            expected_reward = curiosity + 2.0 * avg_update_prob
            expected_reward -= 5.0 * err_prob
        else:
            expected_reward = curiosity
        return expected_reward

    def _sample_tool_action(self) -> Tuple[str, Tuple[PolicyOption, PolicyOption, PolicyOption]]:
        if self.config.tool_name != "auto":
            tool_name = self.config.tool_name
            policy = self.policy.policies[tool_name]
            return tool_name, policy.sample()
        return self.policy.sample()

    def run_episode(
        self, steps: Optional[int] = None, executor: Optional[CommandExecutor] = None
    ) -> None:
        target_steps = steps or self.config.max_steps
        for _ in range(target_steps):
            self.step(executor=executor)

    def _record_success(self) -> None:
        elapsed = time.time() - self._last_reset_time
        self.state.flag_log.append(
            {
                "episode": self.state.episode,
                "step": self.state.step,
                "elapsed_sec": elapsed,
                "history_len": len(self.state.history),
            }
        )

    def _reset_episode(self) -> None:
        self.knowledge.reset()
        self.reward_calc.reset_episode()
        self.state.step = 0
        self.state.episode += 1
        self.state.history.clear()
        self._last_reset_time = time.time()

    def set_step_hook(self, hook: Optional[Callable[[Dict[str, object]], None]]) -> None:
        self._step_hook = hook

    @property
    def executor(self) -> CommandExecutor:
        return self._executor

    @executor.setter
    def executor(self, func: CommandExecutor) -> None:
        self._executor = func

    def _resolve_param(self, kk: str, value: str) -> str:
        """Resolve a policy parameter using knowledge and fallbacks for placeholders."""

        def _fetch(key: str) -> str:
            stored = self.knowledge.store.get(key, [])
            if stored:
                return stored[-1]
            return self._fallback_for(key)

        resolved = value
        placeholders = [p for p in resolved.split() if p.startswith("{") and p.endswith("}")]
        # Replace each placeholder token individually to keep mixed values intact
        for token in placeholders:
            key = token.strip("{}")
            replacement = _fetch(key)
            resolved = resolved.replace(token, replacement)

        if resolved.startswith("{") and resolved.endswith("}"):
            # Entire token is a placeholder but was not replaced (no data)
            key = resolved.strip("{}")
            return _fetch(key)
        return resolved

    def _fallback_for(self, key: str) -> str:
        key_upper = key.upper()
        if key_upper in {"TARGET_IP", "TARGET"}:
            return self.config.target_ip
        if key_upper in {"PORT_LIST", "PORT_SPEC"}:
            return ",".join(str(p) for p in FAMOUS_20_PORTS)
        if key_upper == "TOP_PORTS":
            return str(random.choice(DEFAULT_TOP_PORTS_CHOICES))
        if key_upper == "SCRIPT_SET":
            return random.choice(DEFAULT_SCRIPT_SET_CHOICES)
        if key_upper == "PATH_HINT":
            return "/robots.txt"
        if key_upper == "MAX_RETRIES":
            return "2"
        if key_upper == "HOST_TIMEOUT":
            return "30s"
        if key_upper == "SCAN_DELAY":
            return "0ms"
        if key_upper == "MIN_RATE":
            return "0"
        if key_upper == "MAX_RATE":
            return "0"
        if key_upper == "VER_INTENSITY":
            return "7"
        return ""

    @staticmethod
    def _dummy_executor(command: str) -> Tuple[str, bool]:
        """Default executor that returns an empty XML envelope."""
        xml = """<nmaprun><host><status state='up'/><address addr='127.0.0.1'/></host></nmaprun>"""
        return xml, False
