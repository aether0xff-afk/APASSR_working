"""Extract lightweight finding candidates from parser hints and tool output."""
from __future__ import annotations

import hashlib
import re
from dataclasses import dataclass
from typing import Iterable, Mapping, Sequence

from pentesting_rl.novelty_tokens import extract_top_level_path


_ADMIN_PATHS = ("/admin", "/manager", "/dvwa", "/security.php", "/wp-admin")
_SQL_ERROR_PATTERN = re.compile(r"sql syntax|sql error|mysql|postgres|sqlite", re.IGNORECASE)
_STACKTRACE_PATTERN = re.compile(r"traceback|stack trace|exception", re.IGNORECASE)
_DIR_LISTING_PATTERN = re.compile(r"Index of /", re.IGNORECASE)
_DEFAULT_CREDS_PATTERN = re.compile(r"default credentials|admin/admin|root/root", re.IGNORECASE)


@dataclass
class FindingContext:
    run_id: str
    target: str
    step_index: int
    runlog_line: int
    stdout_ref: str | None
    stderr_ref: str | None


def extract_candidates(
    output: str,
    new_values: Mapping[str, Sequence[str]] | None,
    knowledge: Mapping[str, Sequence[str]] | None,
    context: FindingContext,
) -> list[dict[str, object]]:
    candidates: list[dict[str, object]] = []
    new_values = new_values or {}
    knowledge = knowledge or {}

    admin_hits = _match_admin_paths(output, new_values)
    if admin_hits:
        candidates.append(
            _build_candidate(
                context,
                title="관리자/민감 경로 노출",
                category="Info disclosure",
                severity="low",
                description="관리자 또는 민감 경로가 노출된 정황이 관측됨.",
                asset=_asset_from_paths(context.target, admin_hits),
                confidence=0.55,
            )
        )

    robots_paths = list(new_values.get("ROBOTS_DISALLOW", []))
    if robots_paths:
        candidates.append(
            _build_candidate(
                context,
                title="robots.txt 민감 경로 노출",
                category="Info disclosure",
                severity="info",
                description="robots.txt에서 민감 경로가 노출됨.",
                asset=_asset_from_paths(context.target, robots_paths),
                confidence=0.45,
            )
        )

    if _DIR_LISTING_PATTERN.search(output):
        candidates.append(
            _build_candidate(
                context,
                title="디렉토리 리스팅 노출",
                category="Info disclosure",
                severity="medium",
                description="Index of / 패턴이 관측되어 디렉토리 리스팅 가능성이 있음.",
                asset=context.target,
                confidence=0.6,
            )
        )

    if _DEFAULT_CREDS_PATTERN.search(output):
        candidates.append(
            _build_candidate(
                context,
                title="기본 크리덴셜 힌트",
                category="Weak creds",
                severity="medium",
                description="기본 크리덴셜 사용 가능성 힌트가 관측됨.",
                asset=context.target,
                confidence=0.5,
            )
        )

    server_headers = list(new_values.get("HTTP_SERVER_HEADER", []))
    service_versions = list(new_values.get("Service_Versions", []))
    if server_headers or service_versions:
        candidates.append(
            _build_candidate(
                context,
                title="버전 노출",
                category="Info disclosure",
                severity="info",
                description="Server/X-Powered-By/banner 등 버전 정보가 노출됨.",
                asset=context.target,
                confidence=0.4,
            )
        )

    combined_text = "\n".join(
        [
            output,
            *[str(v) for v in new_values.get("HTTP_BODY_SNIPPET", [])],
            *[str(v) for v in new_values.get("Script_Output", [])],
        ]
    )
    if _SQL_ERROR_PATTERN.search(combined_text) or _STACKTRACE_PATTERN.search(combined_text):
        candidates.append(
            _build_candidate(
                context,
                title="에러 정보 누출",
                category="Info disclosure",
                severity="medium",
                description="SQL 오류 또는 스택트레이스 패턴이 관측됨.",
                asset=context.target,
                confidence=0.65,
            )
        )

    return candidates


def _build_candidate(
    context: FindingContext,
    title: str,
    category: str,
    severity: str,
    description: str,
    asset: str,
    confidence: float,
) -> dict[str, object]:
    finding_id = _finding_id(context.run_id, title, context.step_index)
    evidence_ref: list[dict[str, object]] = [
        {"kind": "runlog", "runlog_line": context.runlog_line},
    ]
    if context.stdout_ref:
        evidence_ref.append({"kind": "artifact", "path": context.stdout_ref})
    if context.stderr_ref:
        evidence_ref.append({"kind": "artifact", "path": context.stderr_ref})
    return {
        "finding_id": finding_id,
        "title": title,
        "severity": severity,
        "category": category,
        "asset": asset,
        "description": description,
        "impact": "정보 노출/공격 표면 증가 가능성.",
        "reproduction_steps": [
            f"{context.target} 대상에 대해 관련 경로를 요청하여 응답을 확인한다."
        ],
        "evidence_ref": evidence_ref,
        "recommendation": "노출 경로 차단, 인증 강화, 에러 메시지 최소화 등을 적용한다.",
        "confidence": confidence,
        "status": "candidate",
    }


def _finding_id(run_id: str, title: str, step_index: int) -> str:
    digest = hashlib.sha256(f"{run_id}:{title}:{step_index}".encode("utf-8")).hexdigest()
    return f"finding_{digest[:12]}"


def _match_admin_paths(output: str, new_values: Mapping[str, Sequence[str]]) -> list[str]:
    hits: list[str] = []
    lowered = output.lower()
    for path in _ADMIN_PATHS:
        if path.lower() in lowered:
            hits.append(path)
    for path in new_values.get("PATH_HINT", []):
        normalized = extract_top_level_path(str(path))
        if normalized in _ADMIN_PATHS:
            hits.append(normalized)
    return sorted(set(hits))


def _asset_from_paths(target: str, paths: Iterable[str]) -> str:
    path_list = list(paths)
    if not path_list:
        return target
    return f"{target}{path_list[0]}"
