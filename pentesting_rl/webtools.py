"""Internal web tools executed via the pytool adapter (local-only)."""
from __future__ import annotations

import base64
import hashlib
import ipaddress
import json
import re
import ssl
import socket
import time
import urllib.error
import urllib.parse
import urllib.request
from html.parser import HTMLParser
from typing import Dict, Iterable, List, Tuple


SCHEMA_VERSION = "webtool-v1"
FLAG_PATTERN = re.compile(r"FLAG\{[^}]{1,128}\}")
PATH_PATTERN = re.compile(r"/[A-Za-z0-9._~!$&'()*+,;=:@/%-]{1,128}")
TITLE_PATTERN = re.compile(r"<title[^>]*>(.*?)</title>", re.IGNORECASE | re.DOTALL)
COMMENT_PATTERN = re.compile(r"<!--(.*?)-->", re.DOTALL)
JWT_PATTERN = re.compile(
    r"eyJ[A-Za-z0-9_-]{10,}\.[A-Za-z0-9_-]{10,}\.[A-Za-z0-9_-]{10,}"
)
B64_PATTERN = re.compile(r"(?:[A-Za-z0-9+/]{16,}={0,2})")
PARAM_PATTERN = re.compile(r"[?&]([A-Za-z0-9_]{1,32})=")
REDIRECT_STATUSES = {301, 302, 303, 307, 308}
BLOCKED_STATUSES = {403, 429}

HINT_KEYWORDS = [
    "flag",
    "todo",
    "debug",
    "backup",
    "secret",
    "password",
    "passwd",
    "token",
    "apikey",
    "api_key",
    "credential",
    "admin",
    "test",
    "dev",
]
ERROR_KEYWORDS = ["error", "exception", "traceback", "stack", "fatal"]

WORDLIST_SMALL = [
    "admin",
    "login",
    "robots.txt",
    "sitemap.xml",
    "flag",
    "secret",
    "backup",
    "api",
    "health",
    "status",
    "debug",
    "test",
    "uploads",
    "static",
    "config",
    ".git",
    "server-status",
    "console",
    "dashboard",
    "dev",
    "staging",
    "old",
    "v1",
    "v2",
    "docs",
    "swagger",
    "graphql",
    "admin.php",
    "login.php",
    "index.php",
    "info.php",
    "phpinfo.php",
    "admin.jsp",
    "login.jsp",
    "admin.aspx",
    "login.aspx",
    "backup.zip",
    "db.sql",
    ".env",
    ".env.bak",
    "config.yml",
    "config.json",
    "secrets.txt",
    "readme.txt",
    "notes.txt",
    "tmp",
    "temp",
    "private",
    "hidden",
]

WORDLIST_MEDIUM = WORDLIST_SMALL + [
    "admin/backup",
    "admin/config",
    "admin/console",
    "admin/health",
    "api/v1",
    "api/v2",
    "api/users",
    "api/admin",
    "auth",
    "auth/login",
    "auth/register",
    "oauth",
    "logout",
    "register",
    "signin",
    "signup",
    "reset",
    "forgot",
    "profile",
    "settings",
    "user",
    "users",
    "me",
    "account",
    "billing",
    "vault",
    "keys",
    "token",
    "tokens",
    "reports",
    "logs",
    "metrics",
    "monitor",
    "trace",
    "error",
    "errors",
    "healthz",
    "ready",
    "live",
    "status/health",
    "swagger.json",
    "openapi.json",
    "swagger-ui",
    "redoc",
    "actuator",
    "actuator/health",
    "actuator/env",
    "actuator/metrics",
    "api-docs",
    "adminer",
    "phpmyadmin",
    "pma",
    "db",
    "database",
    "dump",
    "backup.sql",
    "backup.tar.gz",
    "config.php",
    "settings.php",
    "env",
    "env.local",
    "env.prod",
    "env.dev",
    ".git/config",
    ".git/HEAD",
    ".htaccess",
    ".htpasswd",
    "logs/app.log",
    "logs/error.log",
    "debug.log",
]

WORDLIST_LARGE = WORDLIST_MEDIUM + [
    "admin.old",
    "admin.bak",
    "backup.old",
    "backup.bak",
    "db.bak",
    "db.dump",
    "db.backup",
    "db.sqlite",
    "db.sqlite3",
    "dump.sql",
    "dump.tar.gz",
    "backup.tar",
    "backup.7z",
    "data",
    "data.zip",
    "data.tar.gz",
    "private.zip",
    "private.tar.gz",
    "www.zip",
    "www.tar.gz",
    "site.zip",
    "site.tar.gz",
    "config.old",
    "config.bak",
    "settings.old",
    "settings.bak",
    "package.json",
    "package-lock.json",
    "composer.json",
    "composer.lock",
    "yarn.lock",
    "webpack.config.js",
    "vite.config.js",
    "docker-compose.yml",
    "Dockerfile",
    "kube.yaml",
    "k8s.yaml",
    "secrets.json",
    "secret.txt",
    "flag.txt",
    "flags.txt",
    "readme.md",
    "notes.md",
]


class LinkExtractor(HTMLParser):
    def __init__(self) -> None:
        super().__init__()
        self.links: List[str] = []
        self.forms: List[str] = []
        self.scripts: List[str] = []

    def handle_starttag(self, tag: str, attrs: List[Tuple[str, str | None]]) -> None:
        attrs_dict = {k: v for k, v in attrs if v is not None}
        if tag in {"a", "link"} and "href" in attrs_dict:
            self.links.append(attrs_dict["href"])
        if tag == "form" and "action" in attrs_dict:
            self.forms.append(attrs_dict["action"])
        if tag == "script" and "src" in attrs_dict:
            self.scripts.append(attrs_dict["src"])


def _new_cost(requests: int = 0, time_ms: int = 0, errors: int = 0) -> Dict[str, int]:
    return {"requests": requests, "time_ms": time_ms, "errors": errors}


def _new_safety(scope_violation: bool = False, blocked: bool = False) -> Dict[str, bool]:
    return {"scope_violation": scope_violation, "blocked": blocked}


def _base_response(
    ok: bool,
    error: str | None,
    observations: Dict[str, object],
    artifacts: Dict[str, object],
    cost: Dict[str, int],
    safety: Dict[str, bool],
) -> Dict[str, object]:
    payload: Dict[str, object] = {
        "schema_version": SCHEMA_VERSION,
        "ok": ok,
        "error": error,
        "observations": observations,
        "artifacts": artifacts,
        "cost": cost,
        "safety": safety,
    }
    return payload


def _error_response(message: str, scope_violation: bool = False) -> Dict[str, object]:
    return _base_response(
        ok=False,
        error=message,
        observations={},
        artifacts={},
        cost=_new_cost(errors=1),
        safety=_new_safety(scope_violation=scope_violation, blocked=False),
    )


def _headers_present(headers: Dict[str, str]) -> List[str]:
    return sorted({str(key) for key in headers.keys()})


def _extract_params_from_urls(urls: Iterable[str]) -> List[str]:
    params: List[str] = []
    for url in urls:
        for name in PARAM_PATTERN.findall(url or ""):
            if name not in params:
                params.append(name)
    return params


def _extract_params_from_text(text: str) -> List[str]:
    return _dedupe(PARAM_PATTERN.findall(text or ""))


def _cookie_names(values: Iterable[str]) -> List[str]:
    names: List[str] = []
    for value in values:
        name = str(value).split("=", 1)[0].strip()
        if name and name not in names:
            names.append(name)
    return names


def execute_pytool(tokens: List[str]) -> Dict[str, object]:
    if len(tokens) < 3:
        return _error_response("pytool requires name and payload")
    tool_name = tokens[1]
    try:
        payload = json.loads(base64.urlsafe_b64decode(tokens[2]).decode("utf-8"))
    except Exception as exc:  # pragma: no cover - defensive
        return _error_response(f"invalid payload: {exc}")

    try:
        if tool_name == "http-fetch":
            return http_fetch(payload)
        if tool_name == "robots-sitemap":
            return robots_sitemap(payload)
        if tool_name == "html-crawler":
            return html_crawler(payload)
        if tool_name == "dir-enum":
            return dir_enum(payload)
        if tool_name == "hint-scanner":
            return hint_scanner(payload)
        if tool_name == "stateful-http":
            return stateful_http(payload)
        if tool_name == "param-influence":
            return param_influence(payload)
    except Exception as exc:  # pragma: no cover - keep tool stable
        return _error_response(str(exc))

    return _error_response(f"unknown tool '{tool_name}'")


def http_fetch(payload: Dict[str, object]) -> Dict[str, object]:
    target_ip = str(payload.get("target_ip", "")).strip()
    port = str(payload.get("port", "80")).strip() or "80"
    path = str(payload.get("path", "/")).strip() or "/"
    method = str(payload.get("method", "GET")).upper()
    headers = payload.get("headers") or {}
    timeout = _as_int(payload.get("timeout"), 5)
    follow_redirects = _as_bool(payload.get("follow_redirects"), True)
    max_redirects = _as_int(payload.get("max_redirects"), 3)
    max_bytes = _as_int(payload.get("max_bytes"), 200_000)
    scheme = str(payload.get("scheme", "http")).strip() or "http"

    ok, resolved_host, error = _ensure_local_target(target_ip)
    if not ok:
        return _error_response(error or "target not allowed", scope_violation=True)

    url = _build_url(resolved_host, port, path, scheme=scheme)
    result = _fetch_url(
        url,
        method=method,
        headers=_merge_headers(headers),
        timeout=timeout,
        follow_redirects=follow_redirects,
        max_redirects=max_redirects,
        max_bytes=max_bytes,
    )

    request_count = int(result.get("request_count", 0) or 0)
    time_ms = int((result.get("elapsed", 0.0) or 0.0) * 1000)
    errors = 0 if result.get("ok") else 1
    cost = _new_cost(requests=request_count, time_ms=time_ms, errors=errors)

    if not result.get("ok"):
        return _base_response(
            ok=False,
            error=str(result.get("error", "request failed")),
            observations={},
            artifacts={},
            cost=cost,
            safety=_new_safety(scope_violation=False, blocked=False),
        )

    body = result.get("body", b"")
    headers_out = result.get("headers", {})
    text = _decode_body(body, headers_out.get("Content-Type"))
    title = _extract_title(text)
    snippet = _snippet(text)
    paths = _extract_paths(text)
    params = _dedupe(_extract_params_from_urls(paths) + _extract_params_from_text(text))
    cookies = result.get("set_cookies", [])
    cookie_names = _cookie_names(cookies)
    status_code = result.get("status_code")
    blocked = status_code in BLOCKED_STATUSES

    observations = {
        "http_status": [status_code] if status_code is not None else [],
        "headers_present": _headers_present(headers_out),
        "forms_detected": False,
        "params_detected": params,
        "redirect_depth": len(result.get("redirects", [])),
        "title": title,
        "content_type": headers_out.get("Content-Type", ""),
        "content_length": _content_length(result, body),
        "body_sha1": hashlib.sha1(body).hexdigest() if body else "",
        "body_snippet": snippet,
    }
    artifacts = {
        "urls": paths,
        "params": params,
        "cookies": cookie_names,
        "set_cookies": cookies,
    }

    return _base_response(
        ok=True,
        error=None,
        observations=observations,
        artifacts=artifacts,
        cost=cost,
        safety=_new_safety(scope_violation=False, blocked=blocked),
    )


def robots_sitemap(payload: Dict[str, object]) -> Dict[str, object]:
    target_ip = str(payload.get("target_ip", "")).strip()
    port = str(payload.get("port", "80")).strip() or "80"
    base_path = str(payload.get("base_path", "/")).strip() or "/"
    timeout = _as_int(payload.get("timeout"), 5)
    mode = str(payload.get("mode", "ROBOTS_SITEMAP")).upper()
    max_bytes = _as_int(payload.get("max_bytes"), 200_000)
    scheme = str(payload.get("scheme", "http")).strip() or "http"

    ok, resolved_host, error = _ensure_local_target(target_ip)
    if not ok:
        return _error_response(error or "target not allowed", scope_violation=True)

    total_requests = 0
    total_time_ms = 0
    total_errors = 0
    statuses: List[int] = []
    blocked = False
    max_redirects_seen = 0

    robots_url = _build_url(resolved_host, port, _join_path(base_path, "/robots.txt"), scheme)
    robots_result = _fetch_url(
        robots_url,
        method="GET",
        headers=_merge_headers({}),
        timeout=timeout,
        follow_redirects=True,
        max_redirects=2,
        max_bytes=max_bytes,
    )

    disallow: List[str] = []
    allow: List[str] = []
    sitemaps: List[str] = []
    total_requests += int(robots_result.get("request_count", 0) or 0)
    total_time_ms += int((robots_result.get("elapsed", 0.0) or 0.0) * 1000)
    if not robots_result.get("ok"):
        total_errors += 1
    if robots_result.get("status_code") is not None:
        status = int(robots_result.get("status_code"))
        statuses.append(status)
        if status in BLOCKED_STATUSES:
            blocked = True
    max_redirects_seen = max(max_redirects_seen, len(robots_result.get("redirects", [])))

    if robots_result.get("ok"):
        body = robots_result.get("body", b"")
        text = _decode_body(body, robots_result.get("headers", {}).get("Content-Type"))
        disallow, allow, sitemaps = _parse_robots(text)

    sitemap_urls: List[str] = []
    sitemap_paths: List[str] = []

    if mode != "ROBOTS_ONLY":
        sitemap_candidates = list(sitemaps)
        sitemap_candidates.append(
            _build_url(resolved_host, port, _join_path(base_path, "/sitemap.xml"), scheme)
        )
        for sitemap_url in sitemap_candidates:
            if not sitemap_url:
                continue
            sitemap_result = _fetch_url(
                sitemap_url,
                method="GET",
                headers=_merge_headers({}),
                timeout=timeout,
                follow_redirects=True,
                max_redirects=2,
                max_bytes=max_bytes,
            )
            total_requests += int(sitemap_result.get("request_count", 0) or 0)
            total_time_ms += int((sitemap_result.get("elapsed", 0.0) or 0.0) * 1000)
            if not sitemap_result.get("ok"):
                total_errors += 1
            if sitemap_result.get("status_code") is not None:
                status = int(sitemap_result.get("status_code"))
                statuses.append(status)
                if status in BLOCKED_STATUSES:
                    blocked = True
            max_redirects_seen = max(
                max_redirects_seen, len(sitemap_result.get("redirects", []))
            )
            if not sitemap_result.get("ok"):
                continue
            text = _decode_body(
                sitemap_result.get("body", b""),
                sitemap_result.get("headers", {}).get("Content-Type"),
            )
            urls = _parse_sitemap(text)
            for url in urls:
                if url not in sitemap_urls:
                    sitemap_urls.append(url)
                path = _url_to_path(url)
                if path and path not in sitemap_paths:
                    sitemap_paths.append(path)

    paths: List[str] = []
    for entry in disallow + allow + sitemap_paths:
        if entry and entry not in paths:
            paths.append(entry)

    params = _extract_params_from_urls(sitemap_urls)
    observations = {
        "http_status": statuses,
        "headers_present": [],
        "forms_detected": False,
        "params_detected": params,
        "redirect_depth": max_redirects_seen,
        "robots_disallow_count": len(disallow),
        "sitemap_count": len(sitemap_urls),
    }
    artifacts = {
        "urls": paths,
        "params": params,
        "cookies": [],
        "robots_disallow": disallow,
        "robots_allow": allow,
        "sitemaps": sitemaps,
        "sitemap_urls": sitemap_urls,
        "sitemap_paths": sitemap_paths,
    }
    cost = _new_cost(
        requests=total_requests, time_ms=total_time_ms, errors=total_errors
    )

    return _base_response(
        ok=True,
        error=None,
        observations=observations,
        artifacts=artifacts,
        cost=cost,
        safety=_new_safety(scope_violation=False, blocked=blocked),
    )


def html_crawler(payload: Dict[str, object]) -> Dict[str, object]:
    target_ip = str(payload.get("target_ip", "")).strip()
    port = str(payload.get("port", "80")).strip() or "80"
    path = str(payload.get("path", "/")).strip() or "/"
    timeout = _as_int(payload.get("timeout"), 5)
    max_pages = _as_int(payload.get("max_pages"), 3)
    max_depth = _as_int(payload.get("max_depth"), 1)
    fetch_scripts = _as_bool(payload.get("fetch_scripts"), False)
    max_scripts = _as_int(payload.get("max_scripts"), 3)
    scheme = str(payload.get("scheme", "http")).strip() or "http"

    ok, resolved_host, error = _ensure_local_target(target_ip)
    if not ok:
        return _error_response(error or "target not allowed", scope_violation=True)

    base_url = _build_url(resolved_host, port, "/", scheme)
    start_url = _build_url(resolved_host, port, path, scheme)

    visited: Dict[str, int] = {}
    to_visit: List[Tuple[str, int]] = [(start_url, 0)]
    links: List[str] = []
    forms: List[str] = []
    scripts: List[str] = []
    js_endpoints: List[str] = []
    statuses: List[int] = []
    total_requests = 0
    total_time_ms = 0
    total_errors = 0
    max_redirects_seen = 0
    blocked = False

    while to_visit and len(visited) < max_pages:
        url, depth = to_visit.pop(0)
        if url in visited:
            continue
        visited[url] = depth
        result = _fetch_url(
            url,
            method="GET",
            headers=_merge_headers({}),
            timeout=timeout,
            follow_redirects=True,
            max_redirects=2,
            max_bytes=120_000,
        )
        total_requests += int(result.get("request_count", 0) or 0)
        total_time_ms += int((result.get("elapsed", 0.0) or 0.0) * 1000)
        max_redirects_seen = max(max_redirects_seen, len(result.get("redirects", [])))
        if not result.get("ok"):
            total_errors += 1
            continue
        status = result.get("status_code")
        if status is not None:
            statuses.append(int(status))
            if status in BLOCKED_STATUSES:
                blocked = True
        body = result.get("body", b"")
        text = _decode_body(body, result.get("headers", {}).get("Content-Type"))
        extractor = LinkExtractor()
        extractor.feed(text)
        for link in extractor.links:
            normalized = _normalize_to_path(link)
            if normalized and normalized not in links:
                links.append(normalized)
        for action in extractor.forms:
            normalized = _normalize_to_path(action)
            if normalized and normalized not in forms:
                forms.append(normalized)
        for script in extractor.scripts:
            normalized = _normalize_to_path(script)
            if normalized and normalized not in scripts:
                scripts.append(normalized)

        if depth < max_depth:
            for candidate in extractor.links + extractor.forms:
                absolute = _to_absolute_url(candidate, base_url)
                if absolute and absolute not in visited and _is_same_origin(absolute, base_url):
                    to_visit.append((absolute, depth + 1))

    if fetch_scripts:
        count = 0
        for script_path in scripts:
            if count >= max_scripts:
                break
            script_url = _build_url(resolved_host, port, script_path, scheme)
            result = _fetch_url(
                script_url,
                method="GET",
                headers=_merge_headers({}),
                timeout=timeout,
                follow_redirects=True,
                max_redirects=1,
                max_bytes=120_000,
            )
            total_requests += int(result.get("request_count", 0) or 0)
            total_time_ms += int((result.get("elapsed", 0.0) or 0.0) * 1000)
            max_redirects_seen = max(max_redirects_seen, len(result.get("redirects", [])))
            if not result.get("ok"):
                total_errors += 1
                continue
            status = result.get("status_code")
            if status is not None:
                statuses.append(int(status))
                if status in BLOCKED_STATUSES:
                    blocked = True
            count += 1
            text = _decode_body(result.get("body", b""), result.get("headers", {}).get("Content-Type"))
            for endpoint in _extract_js_endpoints(text):
                if endpoint not in js_endpoints:
                    js_endpoints.append(endpoint)

    urls = _dedupe(links + forms + scripts + js_endpoints)
    params = _extract_params_from_urls(urls)
    observations = {
        "http_status": _dedupe([int(code) for code in statuses]),
        "headers_present": [],
        "forms_detected": bool(forms),
        "params_detected": params,
        "redirect_depth": max_redirects_seen,
    }
    artifacts = {
        "urls": urls,
        "params": params,
        "cookies": [],
        "links": links,
        "forms": forms,
        "scripts": scripts,
        "js_endpoints": js_endpoints,
    }
    cost = _new_cost(
        requests=total_requests, time_ms=total_time_ms, errors=total_errors
    )

    return _base_response(
        ok=True,
        error=None,
        observations=observations,
        artifacts=artifacts,
        cost=cost,
        safety=_new_safety(scope_violation=False, blocked=blocked),
    )


def dir_enum(payload: Dict[str, object]) -> Dict[str, object]:
    target_ip = str(payload.get("target_ip", "")).strip()
    port = str(payload.get("port", "80")).strip() or "80"
    base_path = str(payload.get("base_path", "/")).strip() or "/"
    timeout = _as_int(payload.get("timeout"), 4)
    method = str(payload.get("method", "HEAD")).upper()
    wordlist_name = str(payload.get("wordlist", "small")).lower()
    max_paths = _as_int(payload.get("max_paths"), 0)
    scheme = str(payload.get("scheme", "http")).strip() or "http"

    ok, resolved_host, error = _ensure_local_target(target_ip)
    if not ok:
        return _error_response(error or "target not allowed", scope_violation=True)

    wordlist = _get_wordlist(wordlist_name)
    if max_paths > 0:
        wordlist = wordlist[:max_paths]

    found: List[Dict[str, object]] = []
    interesting = {200, 204, 301, 302, 307, 308, 401, 403}
    total_requests = 0
    total_time_ms = 0
    total_errors = 0
    blocked = False
    statuses: List[int] = []
    for word in wordlist:
        candidate_path = _join_path(base_path, word)
        url = _build_url(resolved_host, port, candidate_path, scheme)
        result = _fetch_url(
            url,
            method=method,
            headers=_merge_headers({}),
            timeout=timeout,
            follow_redirects=False,
            max_redirects=0,
            max_bytes=10_000,
        )
        total_requests += int(result.get("request_count", 0) or 0)
        total_time_ms += int((result.get("elapsed", 0.0) or 0.0) * 1000)
        if not result.get("ok"):
            total_errors += 1
        status = result.get("status_code")
        if status in {405, 501} and method != "GET":
            result = _fetch_url(
                url,
                method="GET",
                headers=_merge_headers({}),
                timeout=timeout,
                follow_redirects=False,
                max_redirects=0,
                max_bytes=10_000,
            )
            total_requests += int(result.get("request_count", 0) or 0)
            total_time_ms += int((result.get("elapsed", 0.0) or 0.0) * 1000)
            if not result.get("ok"):
                total_errors += 1
            status = result.get("status_code")
        if status in interesting:
            found.append({"path": candidate_path, "status": status})
        if status is not None:
            statuses.append(int(status))
            if status in BLOCKED_STATUSES:
                blocked = True

    paths = [entry.get("path") for entry in found if entry.get("path")]
    params = _extract_params_from_urls(paths)
    observations = {
        "http_status": _dedupe([int(code) for code in statuses]),
        "headers_present": [],
        "forms_detected": False,
        "params_detected": params,
        "redirect_depth": 0,
    }
    artifacts = {
        "urls": paths,
        "params": params,
        "cookies": [],
        "found": found,
    }
    cost = _new_cost(
        requests=total_requests, time_ms=total_time_ms, errors=total_errors
    )
    return _base_response(
        ok=True,
        error=None,
        observations=observations,
        artifacts=artifacts,
        cost=cost,
        safety=_new_safety(scope_violation=False, blocked=blocked),
    )


def hint_scanner(payload: Dict[str, object]) -> Dict[str, object]:
    target_ip = str(payload.get("target_ip", "")).strip()
    port = str(payload.get("port", "80")).strip() or "80"
    path = str(payload.get("path", "/")).strip() or "/"
    timeout = _as_int(payload.get("timeout"), 5)
    max_bytes = _as_int(payload.get("max_bytes"), 200_000)
    scheme = str(payload.get("scheme", "http")).strip() or "http"

    ok, resolved_host, error = _ensure_local_target(target_ip)
    if not ok:
        return _error_response(error or "target not allowed", scope_violation=True)

    url = _build_url(resolved_host, port, path, scheme)
    result = _fetch_url(
        url,
        method="GET",
        headers=_merge_headers({}),
        timeout=timeout,
        follow_redirects=True,
        max_redirects=3,
        max_bytes=max_bytes,
    )
    request_count = int(result.get("request_count", 0) or 0)
    time_ms = int((result.get("elapsed", 0.0) or 0.0) * 1000)
    errors = 0 if result.get("ok") else 1
    cost = _new_cost(requests=request_count, time_ms=time_ms, errors=errors)
    if not result.get("ok"):
        return _base_response(
            ok=False,
            error=str(result.get("error", "request failed")),
            observations={},
            artifacts={},
            cost=cost,
            safety=_new_safety(scope_violation=False, blocked=False),
        )

    text = _decode_body(result.get("body", b""), result.get("headers", {}).get("Content-Type"))
    flags = _dedupe(FLAG_PATTERN.findall(text))
    keywords = _find_keywords(text)
    jwt_hits = _dedupe(JWT_PATTERN.findall(text))
    b64_hits = _dedupe(_filter_base64(B64_PATTERN.findall(text)))
    comments = _extract_comments(text)
    paths = _extract_paths(text)
    params = _dedupe(_extract_params_from_urls(paths) + _extract_params_from_text(text))
    headers_out = result.get("headers", {})
    status_code = result.get("status_code")
    blocked = status_code in BLOCKED_STATUSES

    hints: List[str] = []
    hints.extend([f"flag:{flag}" for flag in flags])
    hints.extend([f"keyword:{kw}" for kw in keywords])
    hints.extend([f"jwt:{jwt}" for jwt in jwt_hits])
    hints.extend([f"base64:{b64}" for b64 in b64_hits])
    hints.extend([f"comment:{comment}" for comment in comments])

    observations = {
        "http_status": [status_code] if status_code is not None else [],
        "headers_present": _headers_present(headers_out),
        "forms_detected": False,
        "params_detected": params,
        "redirect_depth": len(result.get("redirects", [])),
        "body_snippet": _snippet(text),
    }
    artifacts = {
        "urls": paths,
        "params": params,
        "cookies": _cookie_names(result.get("set_cookies", [])),
        "hints": hints,
        "flags": flags,
        "keywords": keywords,
        "jwt": jwt_hits,
        "base64": b64_hits,
        "comments": comments,
    }

    return _base_response(
        ok=True,
        error=None,
        observations=observations,
        artifacts=artifacts,
        cost=cost,
        safety=_new_safety(scope_violation=False, blocked=blocked),
    )


def stateful_http(payload: Dict[str, object]) -> Dict[str, object]:
    target_ip = str(payload.get("target_ip", "")).strip()
    port = str(payload.get("port", "80")).strip() or "80"
    path = str(payload.get("path", "/")).strip() or "/"
    method = str(payload.get("method", "GET")).upper()
    headers = payload.get("headers") or {}
    timeout = _as_int(payload.get("timeout"), 5)
    follow_redirects = _as_bool(payload.get("follow_redirects"), True)
    max_redirects = _as_int(payload.get("max_redirects"), 3)
    max_bytes = _as_int(payload.get("max_bytes"), 200_000)
    scheme = str(payload.get("scheme", "http")).strip() or "http"
    cookie_header = str(payload.get("cookie_header", "")).strip()

    ok, resolved_host, error = _ensure_local_target(target_ip)
    if not ok:
        return _error_response(error or "target not allowed", scope_violation=True)

    url = _build_url(resolved_host, port, path, scheme=scheme)
    merged_headers = _merge_headers(headers)
    if cookie_header:
        merged_headers["Cookie"] = cookie_header

    result = _fetch_url(
        url,
        method=method,
        headers=merged_headers,
        timeout=timeout,
        follow_redirects=follow_redirects,
        max_redirects=max_redirects,
        max_bytes=max_bytes,
    )

    request_count = int(result.get("request_count", 0) or 0)
    time_ms = int((result.get("elapsed", 0.0) or 0.0) * 1000)
    errors = 0 if result.get("ok") else 1
    cost = _new_cost(requests=request_count, time_ms=time_ms, errors=errors)

    if not result.get("ok"):
        return _base_response(
            ok=False,
            error=str(result.get("error", "request failed")),
            observations={},
            artifacts={},
            cost=cost,
            safety=_new_safety(scope_violation=False, blocked=False),
        )

    body = result.get("body", b"")
    headers_out = result.get("headers", {})
    text = _decode_body(body, headers_out.get("Content-Type"))
    title = _extract_title(text)
    snippet = _snippet(text)
    paths = _extract_paths(text)
    params = _dedupe(_extract_params_from_urls(paths) + _extract_params_from_text(text))
    cookies = result.get("set_cookies", [])
    cookie_names = _cookie_names(cookies)
    status_code = result.get("status_code")
    blocked = status_code in BLOCKED_STATUSES

    sent_cookies = _cookie_names(cookie_header.split(";")) if cookie_header else []
    observations = {
        "http_status": [status_code] if status_code is not None else [],
        "headers_present": _headers_present(headers_out),
        "forms_detected": False,
        "params_detected": params,
        "redirect_depth": len(result.get("redirects", [])),
        "title": title,
        "content_type": headers_out.get("Content-Type", ""),
        "content_length": _content_length(result, body),
        "body_sha1": hashlib.sha1(body).hexdigest() if body else "",
        "body_snippet": snippet,
    }
    artifacts = {
        "urls": paths,
        "params": params,
        "cookies": cookie_names,
        "set_cookies": cookies,
        "cookies_sent": sent_cookies,
    }

    return _base_response(
        ok=True,
        error=None,
        observations=observations,
        artifacts=artifacts,
        cost=cost,
        safety=_new_safety(scope_violation=False, blocked=blocked),
    )


def param_influence(payload: Dict[str, object]) -> Dict[str, object]:
    target_ip = str(payload.get("target_ip", "")).strip()
    port = str(payload.get("port", "80")).strip() or "80"
    path = str(payload.get("path", "/")).strip() or "/"
    params = payload.get("params") or []
    timeout = _as_int(payload.get("timeout"), 5)
    max_params = _as_int(payload.get("max_params"), 5)
    scheme = str(payload.get("scheme", "http")).strip() or "http"

    ok, resolved_host, error = _ensure_local_target(target_ip)
    if not ok:
        return _error_response(error or "target not allowed", scope_violation=True)

    if isinstance(params, str):
        params = [p.strip() for p in params.split(",") if p.strip()]
    if not isinstance(params, list):
        params = []
    params = [str(p) for p in params][:max_params]

    base_url = _build_url(resolved_host, port, path, scheme=scheme)
    total_requests = 0
    total_time_ms = 0
    total_errors = 0
    statuses: List[int] = []
    blocked = False

    baseline = _fetch_url(
        base_url,
        method="GET",
        headers=_merge_headers({}),
        timeout=timeout,
        follow_redirects=True,
        max_redirects=2,
        max_bytes=120_000,
    )
    total_requests += int(baseline.get("request_count", 0) or 0)
    total_time_ms += int((baseline.get("elapsed", 0.0) or 0.0) * 1000)
    if not baseline.get("ok"):
        total_errors += 1
        return _base_response(
            ok=False,
            error=str(baseline.get("error", "request failed")),
            observations={},
            artifacts={},
            cost=_new_cost(requests=total_requests, time_ms=total_time_ms, errors=total_errors),
            safety=_new_safety(scope_violation=False, blocked=False),
        )

    base_body = baseline.get("body", b"")
    base_len = len(base_body) if base_body else 0
    status = baseline.get("status_code")
    if status is not None:
        statuses.append(int(status))
        if status in BLOCKED_STATUSES:
            blocked = True

    param_results: List[Dict[str, object]] = []
    response_variance = 0.0
    status_variance = False
    error_disclosure = False

    for name in params:
        url_a = _append_query(base_url, {name: "probeA"})
        url_b = _append_query(base_url, {name: "probeB"})
        res_a = _fetch_url(
            url_a,
            method="GET",
            headers=_merge_headers({}),
            timeout=timeout,
            follow_redirects=True,
            max_redirects=2,
            max_bytes=120_000,
        )
        res_b = _fetch_url(
            url_b,
            method="GET",
            headers=_merge_headers({}),
            timeout=timeout,
            follow_redirects=True,
            max_redirects=2,
            max_bytes=120_000,
        )

        for res in (res_a, res_b):
            total_requests += int(res.get("request_count", 0) or 0)
            total_time_ms += int((res.get("elapsed", 0.0) or 0.0) * 1000)
            if not res.get("ok"):
                total_errors += 1
            status_code = res.get("status_code")
            if status_code is not None:
                statuses.append(int(status_code))
                if status_code in BLOCKED_STATUSES:
                    blocked = True

        len_a = len(res_a.get("body", b"") or b"")
        len_b = len(res_b.get("body", b"") or b"")
        delta = abs(len_a - len_b)
        denom = max(base_len, len_a, len_b, 1)
        variance = delta / denom
        response_variance = max(response_variance, variance)

        status_a = res_a.get("status_code")
        status_b = res_b.get("status_code")
        if status_a != status_b:
            status_variance = True

        text_a = _decode_body(res_a.get("body", b""), res_a.get("headers", {}).get("Content-Type"))
        text_b = _decode_body(res_b.get("body", b""), res_b.get("headers", {}).get("Content-Type"))
        if _error_keywords_present(text_a) or _error_keywords_present(text_b):
            error_disclosure = True

        param_results.append(
            {
                "param": name,
                "len_a": len_a,
                "len_b": len_b,
                "variance": variance,
                "status_a": status_a,
                "status_b": status_b,
            }
        )

    observations = {
        "http_status": _dedupe([int(code) for code in statuses]),
        "headers_present": [],
        "forms_detected": False,
        "params_detected": params,
        "redirect_depth": 0,
        "response_variance": response_variance,
        "status_variance": status_variance,
        "error_disclosure": error_disclosure,
        "input_output_coupling": response_variance > 0.1,
    }
    artifacts = {
        "urls": [],
        "params": params,
        "cookies": [],
        "param_variance": param_results,
    }
    cost = _new_cost(
        requests=total_requests, time_ms=total_time_ms, errors=total_errors
    )

    return _base_response(
        ok=True,
        error=None,
        observations=observations,
        artifacts=artifacts,
        cost=cost,
        safety=_new_safety(scope_violation=False, blocked=blocked),
    )


def _fetch_url(
    url: str,
    method: str,
    headers: Dict[str, str],
    timeout: int,
    follow_redirects: bool,
    max_redirects: int,
    max_bytes: int,
) -> Dict[str, object]:
    redirects: List[Dict[str, object]] = []
    current_url = url
    current_method = method
    start = time.time()
    request_count = 0

    for _ in range(max_redirects + 1):
        try:
            request = urllib.request.Request(current_url, method=current_method, headers=headers)
            context = None
            if current_url.startswith("https://"):
                context = ssl._create_unverified_context()
            with urllib.request.urlopen(request, timeout=timeout, context=context) as response:
                request_count += 1
                status = response.getcode()
                resp_headers = dict(response.headers.items())
                set_cookies = response.headers.get_all("Set-Cookie") or []
                body = response.read(max_bytes) if current_method != "HEAD" else b""
        except urllib.error.HTTPError as exc:
            request_count += 1
            status = exc.code
            resp_headers = dict(exc.headers.items()) if exc.headers else {}
            set_cookies = exc.headers.get_all("Set-Cookie") if exc.headers else []
            body = exc.read(max_bytes) if current_method != "HEAD" else b""
        except Exception as exc:
            return {
                "ok": False,
                "error": str(exc),
                "url": current_url,
                "request_count": request_count,
            }

        location = resp_headers.get("Location")
        if follow_redirects and status in REDIRECT_STATUSES and location:
            redirects.append({"status": status, "location": location})
            current_url = urllib.parse.urljoin(current_url, location)
            if status in {301, 302, 303} and current_method != "HEAD":
                current_method = "GET"
            continue

        return {
            "ok": True,
            "url": current_url,
            "status_code": status,
            "headers": resp_headers,
            "set_cookies": set_cookies,
            "body": body,
            "redirects": redirects,
            "elapsed": time.time() - start,
            "request_count": request_count,
        }

    return {
        "ok": False,
        "error": "redirect limit exceeded",
        "url": current_url,
        "redirects": redirects,
        "request_count": request_count,
    }


def _ensure_local_target(target: str) -> Tuple[bool, str, str | None]:
    host = target.strip()
    if not host:
        return False, "", "missing target"
    try:
        ip_obj = ipaddress.ip_address(host)
    except ValueError:
        try:
            resolved = socket.gethostbyname(host)
            ip_obj = ipaddress.ip_address(resolved)
        except Exception:
            return False, "", "target resolution failed"

    if not (ip_obj.is_private or ip_obj.is_loopback):
        return False, host, "target must be private or loopback"
    return True, host, None


def _build_url(host: str, port: str, path: str, scheme: str = "http") -> str:
    normalized = _normalize_to_path(path) or "/"
    return f"{scheme}://{host}:{port}{normalized}"


def _normalize_to_path(value: str) -> str:
    value = value.strip()
    if not value:
        return ""
    if value.startswith("http://") or value.startswith("https://"):
        parsed = urllib.parse.urlparse(value)
        path = parsed.path or "/"
        if parsed.query:
            path += "?" + parsed.query
        return path
    if value.startswith("/"):
        return value
    return f"/{value}"


def _url_to_path(url: str) -> str:
    try:
        parsed = urllib.parse.urlparse(url)
    except Exception:
        return ""
    path = parsed.path or ""
    if parsed.query:
        path += "?" + parsed.query
    return path


def _join_path(base: str, tail: str) -> str:
    base_norm = _normalize_to_path(base) or "/"
    tail_norm = _normalize_to_path(tail) or "/"
    if base_norm.endswith("/"):
        base_norm = base_norm[:-1]
    return base_norm + tail_norm


def _merge_headers(headers: Dict[str, object]) -> Dict[str, str]:
    merged: Dict[str, str] = {"User-Agent": "RLProbe/1.0"}
    for key, value in headers.items():
        merged[str(key)] = str(value)
    return merged


def _decode_body(body: bytes, content_type: str | None) -> str:
    if not body:
        return ""
    encoding = "utf-8"
    if content_type and "charset=" in content_type:
        encoding = content_type.split("charset=")[-1].split(";")[0].strip()
    try:
        return body.decode(encoding, errors="ignore")
    except Exception:
        return body.decode("utf-8", errors="ignore")


def _extract_title(text: str) -> str:
    match = TITLE_PATTERN.search(text or "")
    if not match:
        return ""
    title = match.group(1)
    return " ".join(title.split())[:120]


def _extract_paths(text: str) -> List[str]:
    return _dedupe(PATH_PATTERN.findall(text or ""))


def _extract_js_endpoints(text: str) -> List[str]:
    if not text:
        return []
    pattern = re.compile(r"/[A-Za-z0-9._~!$&'()*+,;=:@/%-]{2,128}")
    candidates = pattern.findall(text)
    filtered = [c for c in candidates if c.count("/") >= 1]
    return _dedupe(filtered)[:60]


def _parse_robots(text: str) -> Tuple[List[str], List[str], List[str]]:
    disallow: List[str] = []
    allow: List[str] = []
    sitemaps: List[str] = []
    for line in text.splitlines():
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        if line.lower().startswith("disallow:"):
            value = line.split(":", 1)[1].strip()
            if value and value not in disallow:
                disallow.append(value)
        elif line.lower().startswith("allow:"):
            value = line.split(":", 1)[1].strip()
            if value and value not in allow:
                allow.append(value)
        elif line.lower().startswith("sitemap:"):
            value = line.split(":", 1)[1].strip()
            if value and value not in sitemaps:
                sitemaps.append(value)
    return disallow, allow, sitemaps


def _parse_sitemap(text: str) -> List[str]:
    urls: List[str] = []
    if "<loc>" not in text:
        return urls
    for match in re.findall(r"<loc>(.*?)</loc>", text, flags=re.IGNORECASE):
        value = match.strip()
        if value and value not in urls:
            urls.append(value)
    return urls


def _filter_base64(values: Iterable[str]) -> List[str]:
    filtered: List[str] = []
    for value in values:
        if len(value) < 16:
            continue
        if len(value) > 160:
            continue
        if value in filtered:
            continue
        filtered.append(value)
        if len(filtered) >= 20:
            break
    return filtered


def _extract_comments(text: str) -> List[str]:
    comments: List[str] = []
    for match in COMMENT_PATTERN.findall(text or ""):
        cleaned = " ".join(match.split())
        if not cleaned:
            continue
        comments.append(cleaned[:120])
        if len(comments) >= 20:
            break
    return _dedupe(comments)


def _find_keywords(text: str) -> List[str]:
    lowered = text.lower()
    found: List[str] = []
    for keyword in HINT_KEYWORDS:
        if keyword in lowered and keyword not in found:
            found.append(keyword)
    return found


def _snippet(text: str) -> str:
    snippet = " ".join(text.split())
    return snippet[:200]


def _content_length(result: Dict[str, object], body: bytes) -> int:
    header = result.get("headers", {}).get("Content-Length") if result else None
    try:
        return int(header) if header else len(body)
    except (ValueError, TypeError):
        return len(body)


def _is_same_origin(url: str, base_url: str) -> bool:
    try:
        return urllib.parse.urlparse(url).netloc == urllib.parse.urlparse(base_url).netloc
    except Exception:
        return False


def _to_absolute_url(value: str, base_url: str) -> str:
    if not value:
        return ""
    return urllib.parse.urljoin(base_url, value)


def _append_query(url: str, params: Dict[str, str]) -> str:
    if not params:
        return url
    parsed = urllib.parse.urlparse(url)
    query = urllib.parse.parse_qs(parsed.query)
    for key, value in params.items():
        query[key] = [value]
    encoded = urllib.parse.urlencode(query, doseq=True)
    return urllib.parse.urlunparse(
        (parsed.scheme, parsed.netloc, parsed.path, parsed.params, encoded, parsed.fragment)
    )


def _error_keywords_present(text: str) -> bool:
    lowered = text.lower()
    return any(keyword in lowered for keyword in ERROR_KEYWORDS)


def _get_wordlist(name: str) -> List[str]:
    if name == "large":
        return WORDLIST_LARGE
    if name == "medium":
        return WORDLIST_MEDIUM
    return WORDLIST_SMALL


def _as_bool(value: object, default: bool) -> bool:
    if value is None:
        return default
    if isinstance(value, bool):
        return value
    text = str(value).strip().lower()
    if text in {"1", "true", "yes", "y", "on"}:
        return True
    if text in {"0", "false", "no", "n", "off"}:
        return False
    return default


def _as_int(value: object, default: int) -> int:
    try:
        return int(value)  # type: ignore[arg-type]
    except (TypeError, ValueError):
        return default


def _dedupe(items: Iterable[str]) -> List[str]:
    seen: set[str] = set()
    out: List[str] = []
    for item in items:
        if item not in seen:
            seen.add(item)
            out.append(item)
    return out
