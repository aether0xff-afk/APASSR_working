"""Reward calculations for extrinsic and intrinsic signals."""
from __future__ import annotations

import math
from typing import Dict, Iterable

DEFAULT_BETA = 0.3
DEFAULT_LAMBDA_COST = 0.05


class RewardCalculator:
    def __init__(
        self, beta: float = DEFAULT_BETA, lambda_cost: float = DEFAULT_LAMBDA_COST
    ) -> None:
        self.beta = beta
        self.lambda_cost = lambda_cost
        self.combo_counts: Dict[str, int] = {}

    def external_reward(
        self,
        combo_key: str,
        syntax_error: bool,
        crash_error: bool,
        kk_updated: Iterable[str],
        flag_found: bool,
    ) -> float:
        reward = 0.0
        # Curiosity reward for executing a command (discounted if repeated)
        count = self.combo_counts.get(combo_key, 0) + 1
        self.combo_counts[combo_key] = count
        reward += 1 / math.sqrt(count)

        if syntax_error:
            reward -= 5.0
        elif crash_error:
            reward -= 2.0

        if flag_found:
            reward += 50.0

        if any(kk_updated):
            reward += 2.0

        return reward

    def intrinsic_reward(self, loss: float) -> float:
        return self.beta * math.tanh(loss)

    def cost_penalty(self, cost: Dict[str, float] | None) -> float:
        if not cost:
            return 0.0
        requests = float(cost.get("requests", 0.0))
        time_ms = float(cost.get("time_ms", 0.0))
        errors = float(cost.get("errors", 0.0))
        return self.lambda_cost * (requests + errors * 2.0 + time_ms / 1000.0)

    def total_reward(self, ext: float, intrinsic: float, penalty: float = 0.0) -> float:
        return ext + intrinsic - penalty

    def reset_episode(self) -> None:
        self.combo_counts.clear()
