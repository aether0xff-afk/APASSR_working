"""Parser for nmap XML outputs mapped to fixed KK keys."""
from __future__ import annotations

import hashlib
import json
import pathlib
import re
import sys
import urllib.parse
import xml.etree.ElementTree as ET
from dataclasses import dataclass, field
from typing import Dict, List, Set

if __package__ in {None, ""}:
    sys.path.append(str(pathlib.Path(__file__).resolve().parent.parent))

from pentesting_rl.knowledge import FIXED_KK, KnowledgeStorage

FLAG_PATTERN = re.compile(r"FLAG\{[^}]{1,128}\}")
ROBOTS_DISALLOW_PATTERN = re.compile(r"Disallow:\s*(/[^\s#]+)", re.IGNORECASE)
HTTP_STATUS_302_PATTERN = re.compile(r"HTTP/\\d\\.\\d\\s+302\\b", re.IGNORECASE)
HEADER_LOCATION_PATTERN = re.compile(r"^Location:\\s*(\\S+)", re.IGNORECASE | re.MULTILINE)
HEADER_XNEXT_PATTERN = re.compile(r"^X-Next:\\s*(\\S+)", re.IGNORECASE | re.MULTILINE)
PATH_PATTERN = re.compile(r"/[A-Za-z0-9._~!$&'()*+,;=:@/%-]{1,128}")
PATH_BUCKET_COUNT = 8


def _record_path_bucket(value: str, updates: Dict[str, List[str]]) -> None:
    path = value.strip()
    if not path.startswith("/"):
        match = PATH_PATTERN.search(path)
        if not match:
            return
        path = match.group(0)
    digest = hashlib.sha256(path.encode("utf-8")).hexdigest()
    bucket = int(digest[:8], 16) % PATH_BUCKET_COUNT
    updates[f"PATH_SEEN_BUCKET_{bucket}"].append(path)


@dataclass
class ParsedResult:
    updates: Set[str]
    error: bool
    new_values: Dict[str, List[str]]
    cost: Dict[str, int] = field(default_factory=lambda: {"requests": 0, "time_ms": 0, "errors": 0})
    safety: Dict[str, bool] = field(
        default_factory=lambda: {"scope_violation": False, "blocked": False}
    )


class NmapXMLParser:
    """Map nmap -oX output into the fixed KK set."""

    def __init__(self, storage: KnowledgeStorage) -> None:
        self.storage = storage

    def parse_and_update(self, xml_data: str) -> ParsedResult:
        updates: Dict[str, List[str]] = {key: [] for key in FIXED_KK}
        error = False

        try:
            root = ET.fromstring(xml_data)
        except ET.ParseError:
            updates["Errors"].append("XMLParseError")
            updates["RAW_XML"].append(xml_data)
            error = True
            updated_keys, new_values = self.storage.update_many_with_values(updates)
            cost = {"requests": 1, "time_ms": 0, "errors": 1}
            return ParsedResult(updated_keys, error, new_values, cost=cost)

        self._extract_host_info(root, updates)
        self._extract_ports(root, updates)
        self._extract_scripts(root, updates)
        self._extract_raw(root, updates, xml_data)

        updated_keys, new_values = self.storage.update_many_with_values(updates)
        cost = {"requests": 1, "time_ms": 0, "errors": 0}
        return ParsedResult(updated_keys, error, new_values, cost=cost)

    def _extract_host_info(self, root: ET.Element, updates: Dict[str, List[str]]) -> None:
        for host in root.findall("host"):
            status_el = host.find("status")
            if status_el is not None:
                state = status_el.get("state")
                if state:
                    updates["Host_Status"].append(state)
            address_el = host.find("address")
            if address_el is not None:
                addr = address_el.get("addr")
                if addr:
                    updates["Target_IP"].append(addr)
            for osmatch in host.findall("os/osmatch"):
                name = osmatch.get("name")
                if name:
                    updates["Host_OS"].append(name)

    def _extract_ports(self, root: ET.Element, updates: Dict[str, List[str]]) -> None:
        for port_el in root.findall(".//port"):
            portid = port_el.get("portid")
            proto = port_el.get("protocol")
            state_el = port_el.find("state")
            state = state_el.get("state") if state_el is not None else None
            service_el = port_el.find("service")
            service_name = service_el.get("name") if service_el is not None else None
            service_ver = service_el.get("version") if service_el is not None else None

            port_repr = f"{proto}/{portid}" if proto and portid else portid or ""
            if state == "open":
                updates["Open_Ports"].append(port_repr)
            elif state == "closed":
                updates["Closed_Ports"].append(port_repr)
            elif state:
                updates["Filtered_Ports"].append(port_repr)

            if service_name:
                updates["Services"].append(f"{port_repr}:{service_name}")
            if service_ver:
                updates["Service_Versions"].append(f"{port_repr}:{service_ver}")

    def _extract_scripts(self, root: ET.Element, updates: Dict[str, List[str]]) -> None:
        for script in root.findall(".//script"):
            output = script.get("output")
            if output:
                updates["Script_Output"].append(output)
                for match in FLAG_PATTERN.findall(output):
                    updates["Flag"].append(match)
                    updates["FLAG_FOUND"].append(match)
                if "robots" in output.lower():
                    updates["HTTP_ROBOTS_FOUND"].append("true")
                disallows = ROBOTS_DISALLOW_PATTERN.findall(output)
                if disallows:
                    updates["HTTP_ROBOTS_HAS_DISALLOW"].append("true")
                    for path in disallows:
                        self._record_path_bucket(path, updates)
                        updates["PATH_HINT"].append(path)
                if HTTP_STATUS_302_PATTERN.search(output):
                    updates["HTTP_STATUS_302"].append("true")
                location_matches = HEADER_LOCATION_PATTERN.findall(output)
                if location_matches:
                    updates["HTTP_HEADER_LOCATION_PRESENT"].append("true")
                    for value in location_matches:
                        self._record_path_bucket(value, updates)
                        updates["HTTP_HEADER_LOCATION"].append(value)
                        updates["PATH_HINT"].append(value)
                xnext_matches = HEADER_XNEXT_PATTERN.findall(output)
                if xnext_matches:
                    updates["HTTP_HEADER_XNEXT_PRESENT"].append("true")
                    for value in xnext_matches:
                        self._record_path_bucket(value, updates)
                        updates["HTTP_HEADER_XNEXT"].append(value)
                        updates["PATH_HINT"].append(value)
                for path in PATH_PATTERN.findall(output):
                    self._record_path_bucket(path, updates)

    def _extract_raw(self, root: ET.Element, updates: Dict[str, List[str]], xml_data: str) -> None:
        updates["RAW_XML"].append(xml_data)
        kv_dump: List[str] = []
        for tag in ("status", "address", "hostname", "service", "script"):
            for elem in root.findall(f".//{tag}"):
                kv_dump.append(ET.tostring(elem, encoding="unicode"))
        updates["RAW_XML_KV"].extend(kv_dump)
        notes = FLAG_PATTERN.findall(xml_data)
        updates["RAW_NOTES"].extend(notes)
        for match in notes:
            updates["FLAG_FOUND"].append(match)

    def _record_path_bucket(self, value: str, updates: Dict[str, List[str]]) -> None:
        _record_path_bucket(value, updates)


class HTTPHeaderParser:
    """Parse raw HTTP headers into KK updates."""

    def __init__(self, storage: KnowledgeStorage) -> None:
        self.storage = storage

    def parse_and_update(self, header_text: str) -> ParsedResult:
        updates: Dict[str, List[str]] = {key: [] for key in FIXED_KK}
        error = False

        if not header_text.strip():
            updates["Errors"].append("EmptyOutput")
            error = True
            updated_keys, new_values = self.storage.update_many_with_values(updates)
            cost = {"requests": 1, "time_ms": 0, "errors": 1}
            return ParsedResult(updated_keys, error, new_values, cost=cost)

        updates["Script_Output"].append(header_text)

        if HTTP_STATUS_302_PATTERN.search(header_text):
            updates["HTTP_STATUS_302"].append("true")

        for match in FLAG_PATTERN.findall(header_text):
            updates["Flag"].append(match)
            updates["FLAG_FOUND"].append(match)

        location_matches = HEADER_LOCATION_PATTERN.findall(header_text)
        if location_matches:
            updates["HTTP_HEADER_LOCATION_PRESENT"].append("true")
            for value in location_matches:
                self._record_path_bucket(value, updates)
                updates["HTTP_HEADER_LOCATION"].append(value)
                updates["PATH_HINT"].append(value)

        xnext_matches = HEADER_XNEXT_PATTERN.findall(header_text)
        if xnext_matches:
            updates["HTTP_HEADER_XNEXT_PRESENT"].append("true")
            for value in xnext_matches:
                self._record_path_bucket(value, updates)
                updates["HTTP_HEADER_XNEXT"].append(value)
                updates["PATH_HINT"].append(value)

        for path in PATH_PATTERN.findall(header_text):
            self._record_path_bucket(path, updates)

        updated_keys, new_values = self.storage.update_many_with_values(updates)
        cost = {"requests": 1, "time_ms": 0, "errors": 0}
        return ParsedResult(updated_keys, error, new_values, cost=cost)

    def _record_path_bucket(self, value: str, updates: Dict[str, List[str]]) -> None:
        _record_path_bucket(value, updates)


class WebJSONParser:
    """Parse JSON output from internal web tools into KK updates."""

    def __init__(self, storage: KnowledgeStorage) -> None:
        self.storage = storage

    def parse_and_update(self, output: str) -> ParsedResult:
        updates: Dict[str, List[str]] = {key: [] for key in FIXED_KK}
        error = False

        if not output.strip():
            updates["Errors"].append("EmptyOutput")
            error = True
            updated_keys, new_values = self.storage.update_many_with_values(updates)
            cost = {"requests": 0, "time_ms": 0, "errors": 1}
            return ParsedResult(updated_keys, error, new_values, cost=cost)

        try:
            data = json.loads(output)
        except json.JSONDecodeError:
            updates["Errors"].append("JSONParseError")
            updates["RAW_NOTES"].append(output[:400])
            error = True
            updated_keys, new_values = self.storage.update_many_with_values(updates)
            cost = {"requests": 0, "time_ms": 0, "errors": 1}
            return ParsedResult(updated_keys, error, new_values, cost=cost)

        if not isinstance(data, dict):
            updates["Errors"].append("InvalidOutput")
            error = True
            updated_keys, new_values = self.storage.update_many_with_values(updates)
            cost = {"requests": 0, "time_ms": 0, "errors": 1}
            return ParsedResult(updated_keys, error, new_values, cost=cost)

        cost = self._parse_cost(data.get("cost"))
        safety = self._parse_safety(data.get("safety"))

        if not data.get("ok", True):
            error = True
            err_msg = data.get("error")
            if err_msg:
                updates["Errors"].append(str(err_msg)[:200])

        if safety.get("scope_violation"):
            error = True
            updates["Errors"].append("ScopeViolation")

        if "observations" in data or "artifacts" in data:
            self._ingest_schema_v1(data, updates)
        else:
            self._ingest_legacy(data, updates)

        updated_keys, new_values = self.storage.update_many_with_values(updates)
        return ParsedResult(updated_keys, error, new_values, cost=cost, safety=safety)

    def _parse_cost(self, cost: object) -> Dict[str, int]:
        if isinstance(cost, dict):
            return {
                "requests": int(cost.get("requests", 0) or 0),
                "time_ms": int(cost.get("time_ms", 0) or 0),
                "errors": int(cost.get("errors", 0) or 0),
            }
        return {"requests": 0, "time_ms": 0, "errors": 0}

    def _parse_safety(self, safety: object) -> Dict[str, bool]:
        if isinstance(safety, dict):
            return {
                "scope_violation": bool(safety.get("scope_violation", False)),
                "blocked": bool(safety.get("blocked", False)),
            }
        return {"scope_violation": False, "blocked": False}

    def _ingest_schema_v1(self, data: Dict[str, object], updates: Dict[str, List[str]]) -> None:
        observations = data.get("observations") or {}
        artifacts = data.get("artifacts") or {}

        if isinstance(observations, dict):
            statuses = observations.get("http_status") or []
            if isinstance(statuses, list):
                for status in statuses:
                    updates["HTTP_STATUS_CODE"].append(str(status))
                    if str(status) == "302":
                        updates["HTTP_STATUS_302"].append("true")
            headers_present = observations.get("headers_present") or []
            if isinstance(headers_present, list):
                normalized = {str(h).lower() for h in headers_present}
                for header in headers_present:
                    updates["HEADER_PRESENT"].append(str(header))
                if "location" in normalized:
                    updates["HTTP_HEADER_LOCATION_PRESENT"].append("true")
                if "x-next" in normalized:
                    updates["HTTP_HEADER_XNEXT_PRESENT"].append("true")
                if "set-cookie" in normalized:
                    updates["HTTP_SET_COOKIE"].append("present")
            if observations.get("forms_detected"):
                updates["FORMS_DETECTED"].append("true")
            params = observations.get("params_detected") or []
            if isinstance(params, list):
                for param in params:
                    updates["PARAM_DETECTED"].append(str(param))
            redirect_depth = observations.get("redirect_depth")
            if redirect_depth is not None:
                updates["REDIRECT_CHAIN_DEPTH"].append(str(redirect_depth))
            title = observations.get("title")
            if title:
                updates["HTTP_TITLE"].append(str(title))
            content_type = observations.get("content_type")
            if content_type:
                updates["HTTP_CONTENT_TYPE"].append(str(content_type))
            content_length = observations.get("content_length")
            if content_length is not None:
                updates["HTTP_CONTENT_LENGTH"].append(str(content_length))
            body_sha1 = observations.get("body_sha1")
            if body_sha1:
                updates["HTTP_BODY_SHA1"].append(str(body_sha1))
            snippet = observations.get("body_snippet")
            if snippet:
                updates["HTTP_BODY_SNIPPET"].append(str(snippet))
                updates["Script_Output"].append(str(snippet))
            response_variance = observations.get("response_variance")
            if isinstance(response_variance, (int, float)) and response_variance >= 0.1:
                updates["RESPONSE_STRUCTURE_VARIANCE_HIGH"].append("true")
            if observations.get("status_variance"):
                updates["STATUS_VARIANCE_DETECTED"].append("true")
            if observations.get("error_disclosure"):
                updates["ERROR_DISCLOSURE_PATTERN"].append("true")
            if observations.get("input_output_coupling"):
                updates["INPUT_OUTPUT_COUPLING_DETECTED"].append("true")

        if isinstance(artifacts, dict):
            for cookie in artifacts.get("cookies_sent", []) or []:
                cookie_name = str(cookie).split("=", 1)[0].strip()
                if cookie_name:
                    updates["COOKIE_SEEN"].append(cookie_name)
            for url in artifacts.get("urls", []) or []:
                normalized = self._normalize_path(str(url))
                if normalized:
                    self._add_path(normalized, updates)
                    updates["LINK_FOUND"].append(normalized)
            for param in artifacts.get("params", []) or []:
                updates["PARAM_DETECTED"].append(str(param))
            for cookie in artifacts.get("cookies", []) or []:
                cookie_name = str(cookie).split("=", 1)[0].strip()
                if cookie_name:
                    updates["COOKIE_SEEN"].append(cookie_name)
                updates["HTTP_SET_COOKIE"].append(str(cookie))
            for hint in artifacts.get("hints", []) or []:
                self._record_hint(str(hint), updates)
            for flag in artifacts.get("flags", []) or []:
                updates["Flag"].append(str(flag))
                updates["FLAG_FOUND"].append(str(flag))
            for keyword in artifacts.get("keywords", []) or []:
                updates["HINT_KEYWORD"].append(str(keyword))
            for encoded in artifacts.get("base64", []) or []:
                updates["HINT_BASE64"].append(str(encoded))
            for jwt in artifacts.get("jwt", []) or []:
                updates["HINT_JWT"].append(str(jwt))
            for comment in artifacts.get("comments", []) or []:
                updates["HINT_COMMENT"].append(str(comment))
            for link in artifacts.get("links", []) or []:
                normalized = self._normalize_path(str(link))
                if normalized:
                    updates["LINK_FOUND"].append(normalized)
                    self._add_path(normalized, updates)
            for action in artifacts.get("forms", []) or []:
                normalized = self._normalize_path(str(action))
                if normalized:
                    updates["FORM_ACTION"].append(normalized)
                    self._add_path(normalized, updates)
            for script in artifacts.get("scripts", []) or []:
                normalized = self._normalize_path(str(script))
                if normalized:
                    updates["SCRIPT_SRC"].append(normalized)
                    self._add_path(normalized, updates)
            for endpoint in artifacts.get("js_endpoints", []) or []:
                normalized = self._normalize_path(str(endpoint))
                if normalized:
                    updates["JS_ENDPOINT"].append(normalized)
                    self._add_path(normalized, updates)
            for entry in artifacts.get("robots_disallow", []) or []:
                normalized = self._normalize_path(str(entry))
                if normalized:
                    updates["ROBOTS_DISALLOW"].append(normalized)
                    self._add_path(normalized, updates)
            for entry in artifacts.get("robots_allow", []) or []:
                normalized = self._normalize_path(str(entry))
                if normalized:
                    updates["ROBOTS_ALLOW"].append(normalized)
                    self._add_path(normalized, updates)
            if artifacts.get("robots_disallow") or artifacts.get("robots_allow"):
                updates["HTTP_ROBOTS_FOUND"].append("true")
                if artifacts.get("robots_disallow"):
                    updates["HTTP_ROBOTS_HAS_DISALLOW"].append("true")
            for entry in artifacts.get("sitemap_urls", []) or []:
                url = str(entry)
                updates["SITEMAP_URL"].append(url)
                self._add_path(url, updates)
            for entry in artifacts.get("sitemap_paths", []) or []:
                self._add_path(str(entry), updates)
            for entry in artifacts.get("found", []) or []:
                if not isinstance(entry, dict):
                    continue
                path = entry.get("path")
                if path:
                    self._add_path(str(path), updates)
                status = entry.get("status")
                if status is not None:
                    updates["HTTP_STATUS_CODE"].append(str(status))
            for entry in artifacts.get("set_cookies", []) or []:
                updates["HTTP_SET_COOKIE"].append(str(entry))

        cost = data.get("cost") or {}
        if isinstance(cost, dict):
            updates["REQUEST_COUNT"].append(str(cost.get("requests", 0)))
            updates["TIME_MS"].append(str(cost.get("time_ms", 0)))
            updates["ERROR_COUNT"].append(str(cost.get("errors", 0)))

        safety = data.get("safety") or {}
        if isinstance(safety, dict):
            if safety.get("scope_violation"):
                updates["SCOPE_VIOLATION"].append("true")
            if safety.get("blocked"):
                updates["BLOCKED_DETECTED"].append("true")

    def _ingest_legacy(self, data: Dict[str, object], updates: Dict[str, List[str]]) -> None:
        status_code = data.get("status_code")
        if status_code is not None:
            updates["HTTP_STATUS_CODE"].append(str(status_code))
            if str(status_code) == "302":
                updates["HTTP_STATUS_302"].append("true")

        headers = data.get("headers") or {}
        if isinstance(headers, dict):
            location = headers.get("Location") or headers.get("location")
            if location:
                updates["HTTP_HEADER_LOCATION_PRESENT"].append("true")
                updates["HTTP_HEADER_LOCATION"].append(str(location))
                self._add_path(str(location), updates)
            xnext = headers.get("X-Next") or headers.get("x-next")
            if xnext:
                updates["HTTP_HEADER_XNEXT_PRESENT"].append("true")
                updates["HTTP_HEADER_XNEXT"].append(str(xnext))
                self._add_path(str(xnext), updates)
            server = headers.get("Server") or headers.get("server")
            if server:
                updates["HTTP_SERVER_HEADER"].append(str(server))
            set_cookie = headers.get("Set-Cookie") or headers.get("set-cookie")
            if set_cookie:
                updates["HTTP_SET_COOKIE"].append(str(set_cookie))
            content_type = headers.get("Content-Type") or headers.get("content-type")
            if content_type:
                updates["HTTP_CONTENT_TYPE"].append(str(content_type))

        for cookie in data.get("set_cookies", []) or []:
            cookie_name = str(cookie).split("=", 1)[0].strip()
            updates["HTTP_SET_COOKIE"].append(cookie_name or str(cookie))

        title = data.get("title")
        if title:
            updates["HTTP_TITLE"].append(str(title))

        content_length = data.get("content_length")
        if content_length is not None:
            updates["HTTP_CONTENT_LENGTH"].append(str(content_length))

        body_sha1 = data.get("body_sha1")
        if body_sha1:
            updates["HTTP_BODY_SHA1"].append(str(body_sha1))

        snippet = data.get("body_snippet")
        if snippet:
            updates["HTTP_BODY_SNIPPET"].append(str(snippet))
            updates["Script_Output"].append(str(snippet))

        for redirect in data.get("redirects", []) or []:
            if not isinstance(redirect, dict):
                continue
            if redirect.get("status") == 302:
                updates["HTTP_STATUS_302"].append("true")
            location = redirect.get("location")
            if location:
                updates["HTTP_HEADER_LOCATION_PRESENT"].append("true")
                updates["HTTP_HEADER_LOCATION"].append(str(location))
                self._add_path(str(location), updates)

        for path in data.get("paths", []) or []:
            self._add_path(str(path), updates)

        for entry in data.get("found", []) or []:
            if not isinstance(entry, dict):
                continue
            path = entry.get("path")
            if path:
                self._add_path(str(path), updates)
            status = entry.get("status")
            if status is not None:
                updates["HTTP_STATUS_CODE"].append(str(status))

        for link in data.get("links", []) or []:
            normalized = self._normalize_path(str(link))
            if normalized:
                updates["LINK_FOUND"].append(normalized)
                self._add_path(normalized, updates)

        for action in data.get("forms", []) or []:
            normalized = self._normalize_path(str(action))
            if normalized:
                updates["FORM_ACTION"].append(normalized)
                self._add_path(normalized, updates)

        for script in data.get("scripts", []) or []:
            normalized = self._normalize_path(str(script))
            if normalized:
                updates["SCRIPT_SRC"].append(normalized)
                self._add_path(normalized, updates)

        for endpoint in data.get("js_endpoints", []) or []:
            normalized = self._normalize_path(str(endpoint))
            if normalized:
                updates["JS_ENDPOINT"].append(normalized)
                self._add_path(normalized, updates)

        for entry in data.get("robots_disallow", []) or []:
            normalized = self._normalize_path(str(entry))
            if normalized:
                updates["ROBOTS_DISALLOW"].append(normalized)
                self._add_path(normalized, updates)
        for entry in data.get("robots_allow", []) or []:
            normalized = self._normalize_path(str(entry))
            if normalized:
                updates["ROBOTS_ALLOW"].append(normalized)
                self._add_path(normalized, updates)
        if data.get("robots_disallow") or data.get("robots_allow"):
            updates["HTTP_ROBOTS_FOUND"].append("true")
            if data.get("robots_disallow"):
                updates["HTTP_ROBOTS_HAS_DISALLOW"].append("true")

        for entry in data.get("sitemap_urls", []) or []:
            url = str(entry)
            updates["SITEMAP_URL"].append(url)
            self._add_path(url, updates)
        for entry in data.get("sitemap_paths", []) or []:
            self._add_path(str(entry), updates)

        hints = data.get("hints") or {}
        if isinstance(hints, dict):
            for flag in hints.get("flags", []) or []:
                updates["Flag"].append(str(flag))
                updates["FLAG_FOUND"].append(str(flag))
            for keyword in hints.get("keywords", []) or []:
                updates["HINT_KEYWORD"].append(str(keyword))
            for encoded in hints.get("base64", []) or []:
                updates["HINT_BASE64"].append(str(encoded))
            for jwt in hints.get("jwt", []) or []:
                updates["HINT_JWT"].append(str(jwt))
            for comment in hints.get("comments", []) or []:
                updates["HINT_COMMENT"].append(str(comment))

    def _record_hint(self, hint: str, updates: Dict[str, List[str]]) -> None:
        lowered = hint.lower()
        if lowered.startswith("flag:"):
            value = hint.split(":", 1)[1]
            updates["Flag"].append(value)
            updates["FLAG_FOUND"].append(value)
            return
        if lowered.startswith("keyword:"):
            updates["HINT_KEYWORD"].append(hint.split(":", 1)[1])
            return
        if lowered.startswith("jwt:"):
            updates["HINT_JWT"].append(hint.split(":", 1)[1])
            return
        if lowered.startswith("base64:"):
            updates["HINT_BASE64"].append(hint.split(":", 1)[1])
            return
        if lowered.startswith("comment:"):
            updates["HINT_COMMENT"].append(hint.split(":", 1)[1])
            return
        updates["RAW_NOTES"].append(hint)

    def _normalize_path(self, value: str) -> str:
        value = value.strip()
        if not value:
            return ""
        if value.startswith("http://") or value.startswith("https://"):
            parsed = urllib.parse.urlparse(value)
            path = parsed.path or "/"
            if parsed.query:
                path += "?" + parsed.query
            return path
        if value.startswith("/"):
            return value
        return f"/{value}"

    def _add_path(self, value: str, updates: Dict[str, List[str]]) -> None:
        path = self._normalize_path(value)
        if not path:
            return
        updates["PATH_HINT"].append(path)
        _record_path_bucket(path, updates)
